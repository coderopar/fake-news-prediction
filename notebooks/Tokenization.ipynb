{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdce0215",
   "metadata": {},
   "source": [
    "### Tokenization and Stopword Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afab46ba",
   "metadata": {},
   "source": [
    "1. Importing the required libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c85e30",
   "metadata": {},
   "source": [
    "Before feeding the text into a machine learning model, we need to clean and normalize it. Tokenization splits text into individual words, while stopword removal filters out common words (like “the”, “is”, “and”) that don’t carry much meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7aace88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "# Download NLTK resources (run only once)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638701bd",
   "metadata": {},
   "source": [
    "2. Define preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6eb11655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define stopwords set\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # 1. Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 2. Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # 3. Remove punctuation & stopwords\n",
    "    tokens = [word for word in tokens if word.isalpha()]  # keep only words\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ecaeafa",
   "metadata": {},
   "source": [
    "3. Apply to your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3400acf1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Apply to first few texts to check\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m data[\u001b[33m'\u001b[39m\u001b[33mtokens\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mdata\u001b[49m[\u001b[33m'\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m'\u001b[39m].apply(preprocess_text)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# View example\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(data[[\u001b[33m'\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mtokens\u001b[39m\u001b[33m'\u001b[39m]].head())\n",
      "\u001b[31mNameError\u001b[39m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "# Apply to first few texts to check\n",
    "data['tokens'] = data['text'].apply(preprocess_text)\n",
    "\n",
    "# View example\n",
    "print(data[['text', 'tokens']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9b3938",
   "metadata": {},
   "source": [
    "Result:\n",
    "Each article now has a tokens column containing cleaned word lists. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31f1c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Original Text: \"Breaking News: The economy is improving rapidly!\"\n",
    "Tokens: ['breaking', 'news', 'economy', 'improving', 'rapidly']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5cf03a",
   "metadata": {},
   "source": [
    "This prepares the dataset for vectorization  and later model training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
