{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0e33deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import zipfile\n",
    "from wordcloud import WordCloud\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download resources once\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"punkt_tab\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "nltk.download(\"stopwords\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4ecffc",
   "metadata": {},
   "source": [
    "### **2.2 Load and Preview Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcd1651a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files extracted successfully!\n"
     ]
    }
   ],
   "source": [
    "with zipfile.ZipFile(\"../data/archive.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"unzipped_data\")\n",
    "    \n",
    "print(\"Files extracted successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ce84aa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Fake.csv', 'True.csv']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(\"unzipped_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63fe22ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fake News Dataset: (23481, 4)\n",
      "True News Dataset: (21417, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Donald Trump Sends Out Embarrassing New Year’...</td>\n",
       "      <td>Donald Trump just couldn t wish all Americans ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Drunk Bragging Trump Staffer Started Russian ...</td>\n",
       "      <td>House Intelligence Committee Chairman Devin Nu...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sheriff David Clarke Becomes An Internet Joke...</td>\n",
       "      <td>On Friday, it was revealed that former Milwauk...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 30, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trump Is So Obsessed He Even Has Obama’s Name...</td>\n",
       "      <td>On Christmas day, Donald Trump announced that ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 29, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pope Francis Just Called Out Donald Trump Dur...</td>\n",
       "      <td>Pope Francis used his annual Christmas Day mes...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 25, 2017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0   Donald Trump Sends Out Embarrassing New Year’...   \n",
       "1   Drunk Bragging Trump Staffer Started Russian ...   \n",
       "2   Sheriff David Clarke Becomes An Internet Joke...   \n",
       "3   Trump Is So Obsessed He Even Has Obama’s Name...   \n",
       "4   Pope Francis Just Called Out Donald Trump Dur...   \n",
       "\n",
       "                                                text subject  \\\n",
       "0  Donald Trump just couldn t wish all Americans ...    News   \n",
       "1  House Intelligence Committee Chairman Devin Nu...    News   \n",
       "2  On Friday, it was revealed that former Milwauk...    News   \n",
       "3  On Christmas day, Donald Trump announced that ...    News   \n",
       "4  Pope Francis used his annual Christmas Day mes...    News   \n",
       "\n",
       "                date  \n",
       "0  December 31, 2017  \n",
       "1  December 31, 2017  \n",
       "2  December 30, 2017  \n",
       "3  December 29, 2017  \n",
       "4  December 25, 2017  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_df = pd.read_csv(\"unzipped_data/Fake.csv\")\n",
    "true_df = pd.read_csv(\"unzipped_data/True.csv\")\n",
    "\n",
    "print(\"Fake News Dataset:\", fake_df.shape)\n",
    "print(\"True News Dataset:\", true_df.shape)\n",
    "\n",
    "fake_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09523c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44898, 5)\n",
      "label\n",
      "FAKE    23481\n",
      "TRUE    21417\n",
      "Name: count, dtype: int64\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 44898 entries, 0 to 44897\n",
      "Data columns (total 5 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   title    44898 non-null  object\n",
      " 1   text     44898 non-null  object\n",
      " 2   subject  44898 non-null  object\n",
      " 3   date     44898 non-null  object\n",
      " 4   label    44898 non-null  object\n",
      "dtypes: object(5)\n",
      "memory usage: 1.7+ MB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ben Stein Calls Out 9th Circuit Court: Committ...</td>\n",
       "      <td>21st Century Wire says Ben Stein, reputable pr...</td>\n",
       "      <td>US_News</td>\n",
       "      <td>February 13, 2017</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Trump drops Steve Bannon from National Securit...</td>\n",
       "      <td>WASHINGTON (Reuters) - U.S. President Donald T...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>April 5, 2017</td>\n",
       "      <td>TRUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Puerto Rico expects U.S. to lift Jones Act shi...</td>\n",
       "      <td>(Reuters) - Puerto Rico Governor Ricardo Rosse...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>September 27, 2017</td>\n",
       "      <td>TRUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OOPS: Trump Just Accidentally Confirmed He Le...</td>\n",
       "      <td>On Monday, Donald Trump once again embarrassed...</td>\n",
       "      <td>News</td>\n",
       "      <td>May 22, 2017</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Donald Trump heads for Scotland to reopen a go...</td>\n",
       "      <td>GLASGOW, Scotland (Reuters) - Most U.S. presid...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>June 24, 2016</td>\n",
       "      <td>TRUE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Ben Stein Calls Out 9th Circuit Court: Committ...   \n",
       "1  Trump drops Steve Bannon from National Securit...   \n",
       "2  Puerto Rico expects U.S. to lift Jones Act shi...   \n",
       "3   OOPS: Trump Just Accidentally Confirmed He Le...   \n",
       "4  Donald Trump heads for Scotland to reopen a go...   \n",
       "\n",
       "                                                text       subject  \\\n",
       "0  21st Century Wire says Ben Stein, reputable pr...       US_News   \n",
       "1  WASHINGTON (Reuters) - U.S. President Donald T...  politicsNews   \n",
       "2  (Reuters) - Puerto Rico Governor Ricardo Rosse...  politicsNews   \n",
       "3  On Monday, Donald Trump once again embarrassed...          News   \n",
       "4  GLASGOW, Scotland (Reuters) - Most U.S. presid...  politicsNews   \n",
       "\n",
       "                  date label  \n",
       "0    February 13, 2017  FAKE  \n",
       "1       April 5, 2017   TRUE  \n",
       "2  September 27, 2017   TRUE  \n",
       "3         May 22, 2017  FAKE  \n",
       "4       June 24, 2016   TRUE  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#merge and label\n",
    "\n",
    "#Add a label column\n",
    "fake_df[\"label\"] = \"FAKE\"\n",
    "true_df[\"label\"] = \"TRUE\"\n",
    "\n",
    "#Merge into one dataset\n",
    "df = pd.concat([fake_df, true_df], ignore_index = True)\n",
    "\n",
    "#Shuffle the rows so FAKE and TRUE are mixed\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "#Check the structure\n",
    "print(df.shape)\n",
    "print(df[\"label\"].value_counts())\n",
    "print(df.info())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e382ebb",
   "metadata": {},
   "source": [
    "## **Chapter 3. Data Preparation**\n",
    "In this section, we will "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836a0a9d",
   "metadata": {},
   "source": [
    "### **3.1 Lowercasing & URL removal**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eee104a",
   "metadata": {},
   "source": [
    "**1. Defining Preprocessing Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c825954f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing functions defined!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text_lowercase_url(text):\n",
    "    \"\"\"\n",
    "    MAIN PREPROCESSING FUNCTION:\n",
    "    - Converts text to lowercase\n",
    "    - Removes URLs, hyperlinks, and website addresses\n",
    "    - Handles missing values safely\n",
    "    - Cleans extra whitespace\n",
    "    \"\"\"\n",
    "    # Handle missing values\n",
    "    if pd.isna(text) or text is None:\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to string to ensure consistent processing\n",
    "    text = str(text)\n",
    "    \n",
    "    # COMPREHENSIVE URL REMOVAL PATTERN:\n",
    "    url_pattern = r'https?://\\S+|www\\.\\S+|\\S+\\.(com|org|net|edu|gov|io|co|uk)\\S*|bit\\.ly/\\S+|t\\.co/\\S+'\n",
    "    \n",
    "    # Remove all URLs from text\n",
    "    text = re.sub(url_pattern, '', text)\n",
    "    \n",
    "    # Convert entire text to lowercase for consistency\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Clean up extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "print(\"Preprocessing functions defined!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b50db3",
   "metadata": {},
   "source": [
    "**2. Quality check functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d36a6a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_url(text):\n",
    "    \"\"\"Check if text contains any URLs\"\"\"\n",
    "    url_pattern = r'https?://|www\\.|\\.[a-z]{2,}'\n",
    "    return bool(re.search(url_pattern, str(text).lower()))\n",
    "\n",
    "def count_uppercase(text):\n",
    "    \"\"\"Count uppercase characters in text\"\"\"\n",
    "    return sum(1 for char in str(text) if char.isupper())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb9e716",
   "metadata": {},
   "source": [
    "### **3.2 Remove Non-Alphabetic Characters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1f82acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_RE   = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "HTML_RE  = re.compile(r'<.*?>')\n",
    "NONALPH  = re.compile(r'[^a-z\\s]+')     # keep letters & spaces only\n",
    "WS_RE    = re.compile(r'\\s+')\n",
    "\n",
    "# Defining Preprocessing Function\n",
    "def _keep_alpha_only(text: str) -> str:\n",
    "    text = NONALPH.sub(\" \", text)    # remove non-letters\n",
    "    text = WS_RE.sub(\" \", text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da527af",
   "metadata": {},
   "source": [
    "### **3.3 Lemmatization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32faae38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lemmatizer and stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Define preprocessing + lemmatization function\n",
    "def preprocess_and_lemmatize(text):\n",
    "    if isinstance(text, str):  # make sure it's a string\n",
    "        # Lowercase\n",
    "        text = text.lower()\n",
    "\n",
    "        # Remove punctuation, numbers, special chars\n",
    "        text = re.sub(r'[^a-z\\s]', '', text)\n",
    "\n",
    "        # Tokenize\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "\n",
    "        # Remove stopwords + lemmatize\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "\n",
    "        return \" \".join(tokens)\n",
    "    else:\n",
    "        return \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e6a4d2",
   "metadata": {},
   "source": [
    "### **3.4 Apply Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621c251c",
   "metadata": {},
   "source": [
    "**Defining Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec713ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining function to apply preprocessing\n",
    "def apply_preprocessing(text: str) -> str:\n",
    "    \"\"\"\n",
    "      1) preprocess_text_lowercase_url  [lowercase + URL removal + whitespace clean]\n",
    "      2) _keep_alpha_only                [remove non-alphabetic, collapse spaces]\n",
    "      3) preprocess_and_lemmatize [tokenize, drop stopwords, lemmatize]\n",
    "    \"\"\"\n",
    "    # Step 1 (Teammate 4)\n",
    "    text = preprocess_text_lowercase_url(text)\n",
    "\n",
    "    # Optional (if HTML present in some sources): strip simple tags BEFORE alpha-only\n",
    "    # text = HTML_RE.sub(\" \", text)  # Uncomment if needed\n",
    "\n",
    "    # Step 2 (Teammate 4)\n",
    "    text = _keep_alpha_only(text)\n",
    "\n",
    "    # Step 3 (Teammate 5, adapter)\n",
    "    text = preprocess_and_lemmatize(text)\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c5e64b",
   "metadata": {},
   "source": [
    "**Applying Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9283bdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['title_clean'] = df['title'].apply(apply_preprocessing)\n",
    "df['text_clean'] = df['text'].apply(apply_preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b3c176",
   "metadata": {},
   "source": [
    "### **3.5 Preprocessing Tests**\n",
    "\n",
    "These tests validate that our cleaned columns (`title_clean`, `text_clean`) are\n",
    "correctly produced from the raw text (`title`, `text`) using the\n",
    "`apply_preprocessing` pipeline.\n",
    "\n",
    "### What the tests check\n",
    "1. **Column existence & type**  \n",
    "   - Both `*_clean` columns exist, are pandas Series, and contain strings.\n",
    "\n",
    "2. **Basic cleaning guarantees**  \n",
    "   - No URLs remain in the text.  \n",
    "   - All text is lowercase.  \n",
    "   - Only alphabetic characters and spaces are present.  \n",
    "   - No leading, trailing, or repeated whitespace.  \n",
    "   - No `NaN` values; all entries are strings.\n",
    "\n",
    "3. **Pipeline correctness**  \n",
    "   - Each `*_clean` column equals exactly one pass of `apply_preprocessing` on the raw column.\n",
    "\n",
    "4. **Non-empty output for meaningful inputs**  \n",
    "   - If a raw row has meaningful alphabetic content (not just URLs, HTML, or stopwords),\n",
    "     the cleaned version is not empty.\n",
    "\n",
    "5. **Length sanity**  \n",
    "   - Cleaned text is usually shorter or equal in length compared to raw text.  \n",
    "   - The majority of meaningful rows remain non-empty after preprocessing.\n",
    "\n",
    "6. **Summary statistics**  \n",
    "   - Reports row counts, percentage of non-empty values, median lengths,\n",
    "     and how often cleaned length ≤ raw length.\n",
    "\n",
    "### Why these tests matter\n",
    "They ensure our preprocessing pipeline:\n",
    "- Safely handles edge cases (URLs, HTML, missing values).  \n",
    "- Produces consistent, normalized text suitable for tokenization.  \n",
    "- Doesn’t over-clean and erase meaningful content.  \n",
    "- Stays aligned with the defined stopword/lemmatization rules.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49a4222f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_12848\\1507442140.py:26: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  mask = s.fillna(\"\").str.contains(URL_RE)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_12848\\1507442140.py:26: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  mask = s.fillna(\"\").str.contains(URL_RE)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[TITLE] rows=44898\n",
      "  Non-empty (raw/clean): 100.00% / 99.98%\n",
      "  Median length (raw/clean): 73 / 59\n",
      "  <= length preserved ratio: 100.00%\n",
      "\n",
      "[TEXT] rows=44898\n",
      "  Non-empty (raw/clean): 100.00% / 98.41%\n",
      "  Median length (raw/clean): 2186 / 1476\n",
      "  <= length preserved ratio: 100.00%\n",
      "\n",
      " All dataframe preprocessing checks passed (single-pass contract; stopword-aware meaningfulness).\n"
     ]
    }
   ],
   "source": [
    "# --- Neutralize any previously-defined idempotence checker lingering in memory ---\n",
    "def _assert_idempotent(*args, **kwargs):\n",
    "    return  # disabled by design\n",
    "\n",
    "# --- Patterns ---\n",
    "ALPHA_SPACE_RE = re.compile(r\"^[a-z ]*$\")  # letters + spaces only\n",
    "URL_RE = re.compile(r\"(https?://\\S+|www\\.\\S+|\\S+\\.(com|org|net|edu|gov|io|co|uk)\\S*|bit\\.ly/\\S+|t\\.co/\\S+)\")\n",
    "\n",
    "# --- Helpers ---\n",
    "def _example(series, mask):\n",
    "    m = mask.values if hasattr(mask, \"values\") else mask\n",
    "    idx = np.flatnonzero(m)\n",
    "    if len(idx):\n",
    "        i = idx[0]\n",
    "        try:\n",
    "            return str(series.iloc[i])[:200]\n",
    "        except Exception:\n",
    "            return \"<unavailable>\"\n",
    "    return \"<none>\"\n",
    "\n",
    "def _assert_series_exists_and_string(s: pd.Series, name: str):\n",
    "    assert isinstance(s, pd.Series), f\"{name} is not a pandas Series.\"\n",
    "    assert s.dtype == \"object\" or pd.api.types.is_string_dtype(s), f\"{name} must be string-like dtype.\"\n",
    "\n",
    "def _assert_no_urls(s: pd.Series, name: str):\n",
    "    mask = s.fillna(\"\").str.contains(URL_RE)\n",
    "    assert not mask.any(), f\"{name}: URLs found. Example: { _example(s, mask)!r }\"\n",
    "\n",
    "def _assert_lowercase_only(s: pd.Series, name: str):\n",
    "    mask = s.fillna(\"\").str.contains(r\"[A-Z]\")\n",
    "    assert not mask.any(), f\"{name}: Uppercase letters found. Example: { _example(s, mask)!r }\"\n",
    "\n",
    "def _assert_alpha_space_only(s: pd.Series, name: str):\n",
    "    mask = s.fillna(\"\").apply(lambda x: bool(x) and ALPHA_SPACE_RE.match(x) is None)\n",
    "    assert not mask.any(), f\"{name}: Non alpha/space characters found. Example: { _example(s, mask)!r }\"\n",
    "\n",
    "def _assert_no_extra_whitespace(s: pd.Series, name: str):\n",
    "    s2 = s.fillna(\"\")\n",
    "    leading = s2.str.match(r\"^\\s\")\n",
    "    trailing = s2.str.contains(r\"\\s$\")\n",
    "    doubles  = s2.str.contains(r\"\\s{2,}\")\n",
    "    assert not leading.any(),  f\"{name}: Leading spaces found. Example: { _example(s, leading)!r }\"\n",
    "    assert not trailing.any(), f\"{name}: Trailing spaces found. Example: { _example(s, trailing)!r }\"\n",
    "    assert not doubles.any(),  f\"{name}: Multiple consecutive spaces found. Example: { _example(s, doubles)!r }\"\n",
    "\n",
    "def _assert_no_nans_and_strings(s: pd.Series, name: str):\n",
    "    assert not s.isna().any(), f\"{name}: NaNs present.\"\n",
    "    assert s.map(lambda x: isinstance(x, str)).all(), f\"{name}: Non-string detected.\"\n",
    "\n",
    "def _assert_matches_single_pass(raw: pd.Series, cleaned: pd.Series, name: str, fn):\n",
    "    recomputed = raw.apply(fn)\n",
    "    diff = cleaned != recomputed\n",
    "    if diff.any():\n",
    "        i = np.flatnonzero(diff.values)[0]\n",
    "        raise AssertionError(\n",
    "            f\"{name}: cleaned column != single-pass pipeline.\\n\"\n",
    "            f\"  raw:                       {raw.iloc[i]!r}\\n\"\n",
    "            f\"  cleaned (your column):     {cleaned.iloc[i]!r}\\n\"\n",
    "            f\"  recomputed({fn.__name__}): {recomputed.iloc[i]!r}\"\n",
    "        )\n",
    "\n",
    "# --- Meaningfulness heuristic aligned with your pipeline's stopwords ---\n",
    "# Prefer your NLTK stopword set if present; otherwise use a richer fallback.\n",
    "try:\n",
    "    STOPWORDS_FOR_TEST = set(stop_words)  # uses your actual pipeline's stopwords if defined\n",
    "    if not isinstance(STOPWORDS_FOR_TEST, set):\n",
    "        STOPWORDS_FOR_TEST = set(STOPWORDS_FOR_TEST)\n",
    "except NameError:\n",
    "    STOPWORDS_FOR_TEST = set(\"\"\"\n",
    "    a an the and or but if then else when while is are was were be been being am\n",
    "    to of in on at by for from as that this it its into over under about with\n",
    "    i you he she we they me my mine your yours his her hers our ours their theirs\n",
    "    do does did doing done have has had having be been being not no nor\n",
    "    s t d ll re ve m y\n",
    "    \"\"\".split())\n",
    "\n",
    "def _raw_has_meaningful_letters(s: str) -> bool:\n",
    "    \"\"\"Decide if raw text *should* yield non-empty output after your pipeline.\n",
    "       Mirrors your cleaning (strip URLs/HTML/non-alpha) and uses your stopwords.\n",
    "    \"\"\"\n",
    "    if s is None:\n",
    "        return False\n",
    "    s = re.sub(URL_RE, \" \", str(s))          # strip URLs\n",
    "    s = re.sub(r\"<.*?>\", \" \", s)             # strip simple HTML tags\n",
    "    s = re.sub(r\"[^A-Za-z\\s]+\", \" \", s)      # keep letters + spaces\n",
    "    toks = [t.lower() for t in s.split()]\n",
    "    toks = [t for t in toks if t not in STOPWORDS_FOR_TEST]\n",
    "    return any(len(t) >= 2 for t in toks)\n",
    "\n",
    "def _assert_non_empty_when_input_had_letters(raw: pd.Series, cleaned: pd.Series, name: str):\n",
    "    # Only rows that truly have content beyond your stopwords must remain non-empty.\n",
    "    meaningful = raw.fillna(\"\").map(_raw_has_meaningful_letters)\n",
    "    empties = cleaned == \"\"\n",
    "    bad = meaningful & empties\n",
    "    assert not bad.any(), (\n",
    "        f\"{name}: Became empty despite meaningful alphabetic input. \"\n",
    "        f\"Example raw: { _example(raw, bad)!r }\"\n",
    "    )\n",
    "\n",
    "def _length_sanity(raw: pd.Series, cleaned: pd.Series, name: str, min_non_empty_ratio=0.8):\n",
    "    mask_meaningful = raw.fillna(\"\").map(_raw_has_meaningful_letters)\n",
    "    rl = raw[mask_meaningful].fillna(\"\").str.len()\n",
    "    cl = cleaned[mask_meaningful].fillna(\"\").str.len()\n",
    "    if len(rl) == 0:\n",
    "        return\n",
    "    assert (cl <= rl).mean() >= 0.5, f\"{name}: Too many cleaned rows longer than original (meaningful subset).\"\n",
    "    ratio_nonempty = (cl > 0).mean()\n",
    "    assert ratio_nonempty >= min_non_empty_ratio, (\n",
    "        f\"{name}: Too many empty cleaned rows among meaningful inputs \"\n",
    "        f\"(ratio_nonempty={ratio_nonempty:.2f}).\"\n",
    "    )\n",
    "\n",
    "# --- Run checks on both columns ---\n",
    "for raw_col, clean_col in [(\"title\", \"title_clean\"), (\"text\", \"text_clean\")]:\n",
    "    assert raw_col in df.columns,   f\"Missing original column: {raw_col}\"\n",
    "    assert clean_col in df.columns, f\"Missing cleaned column: {clean_col}\"\n",
    "\n",
    "    s_raw = df[raw_col]\n",
    "    s_cln = df[clean_col]\n",
    "\n",
    "    _assert_series_exists_and_string(s_cln, clean_col)\n",
    "    _assert_no_urls(s_cln, clean_col)\n",
    "    _assert_lowercase_only(s_cln, clean_col)\n",
    "    _assert_alpha_space_only(s_cln, clean_col)\n",
    "    _assert_no_extra_whitespace(s_cln, clean_col)\n",
    "    _assert_no_nans_and_strings(s_cln, clean_col)\n",
    "\n",
    "    # Authoritative: cleaned equals ONE single pass over raw\n",
    "    _assert_matches_single_pass(s_raw, s_cln, clean_col, apply_preprocessing)\n",
    "\n",
    "    # Heuristic-based expectations (aligned with your stopwords)\n",
    "    _assert_non_empty_when_input_had_letters(s_raw, s_cln, clean_col)\n",
    "    _length_sanity(s_raw, s_cln, clean_col, min_non_empty_ratio=0.8)\n",
    "\n",
    "# --- Brief summary ---\n",
    "def _summary_block(name: str, raw: pd.Series, clean: pd.Series):\n",
    "    rl = raw.fillna(\"\").str.len()\n",
    "    cl = clean.fillna(\"\").str.len()\n",
    "    print(f\"\\n[{name}] rows={len(raw)}\")\n",
    "    print(f\"  Non-empty (raw/clean): {(rl>0).mean():.2%} / {(cl>0).mean():.2%}\")\n",
    "    print(f\"  Median length (raw/clean): {int(rl.median() if len(rl) else 0)} / {int(cl.median() if len(cl) else 0)}\")\n",
    "    print(f\"  <= length preserved ratio: {(cl<=rl).mean():.2%}\")\n",
    "\n",
    "_summary_block(\"TITLE\", df[\"title\"], df[\"title_clean\"])\n",
    "_summary_block(\"TEXT\",  df[\"text\"],  df[\"text_clean\"])\n",
    "\n",
    "print(\"\\n All dataframe preprocessing checks passed (single-pass contract; stopword-aware meaningfulness).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ccfdc590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset cleaned: dropped 716 empty rows, remaining 44182 rows.\n"
     ]
    }
   ],
   "source": [
    "# 1. Fill NaN with empty strings\n",
    "df[\"text_clean\"] = df[\"text_clean\"].fillna(\"\")\n",
    "df[\"text_clean\"] = df[\"text_clean\"].fillna(\"\")\n",
    "# 2. Convert everything to string and strip whitespace\n",
    "df[\"text_clean\"] = df[\"text_clean\"].astype(str).str.strip()\n",
    "df[\"text_clean\"] = df[\"text_clean\"].astype(str).str.strip()\n",
    "# 3. Drop rows where the text column is empty after cleaning\n",
    "before = len(df)\n",
    "df = df[df[\"text_clean\"].str.len() > 0].copy()\n",
    "df = df[df[\"title_clean\"].str.len() > 0].copy()\n",
    "dropped = before - len(df)\n",
    "\n",
    "print(f\"Dataset cleaned: dropped {dropped} empty rows, remaining {len(df)} rows.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb80bbc",
   "metadata": {},
   "source": [
    "### **3.5 Creating `combined_text` column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3891e6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"combined_text\"] = (df[\"title_clean\"] + \" \" + df[\"text_clean\"]).str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4dc29be5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>label</th>\n",
       "      <th>title_clean</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>combined_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ben Stein Calls Out 9th Circuit Court: Committ...</td>\n",
       "      <td>21st Century Wire says Ben Stein, reputable pr...</td>\n",
       "      <td>US_News</td>\n",
       "      <td>February 13, 2017</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>ben stein call th circuit court committed coup...</td>\n",
       "      <td>st century wire say ben stein reputable profes...</td>\n",
       "      <td>ben stein call th circuit court committed coup...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Trump drops Steve Bannon from National Securit...</td>\n",
       "      <td>WASHINGTON (Reuters) - U.S. President Donald T...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>April 5, 2017</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>trump drop steve bannon national security council</td>\n",
       "      <td>washington reuters u president donald trump re...</td>\n",
       "      <td>trump drop steve bannon national security coun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Puerto Rico expects U.S. to lift Jones Act shi...</td>\n",
       "      <td>(Reuters) - Puerto Rico Governor Ricardo Rosse...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>September 27, 2017</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>puerto rico expects u lift jones act shipping ...</td>\n",
       "      <td>reuters puerto rico governor ricardo rossello ...</td>\n",
       "      <td>puerto rico expects u lift jones act shipping ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OOPS: Trump Just Accidentally Confirmed He Le...</td>\n",
       "      <td>On Monday, Donald Trump once again embarrassed...</td>\n",
       "      <td>News</td>\n",
       "      <td>May 22, 2017</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>oops trump accidentally confirmed leaked israe...</td>\n",
       "      <td>monday donald trump embarrassed country accide...</td>\n",
       "      <td>oops trump accidentally confirmed leaked israe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Donald Trump heads for Scotland to reopen a go...</td>\n",
       "      <td>GLASGOW, Scotland (Reuters) - Most U.S. presid...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>June 24, 2016</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>donald trump head scotland reopen golf resort</td>\n",
       "      <td>glasgow scotland reuters u presidential candid...</td>\n",
       "      <td>donald trump head scotland reopen golf resort ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Ben Stein Calls Out 9th Circuit Court: Committ...   \n",
       "1  Trump drops Steve Bannon from National Securit...   \n",
       "2  Puerto Rico expects U.S. to lift Jones Act shi...   \n",
       "3   OOPS: Trump Just Accidentally Confirmed He Le...   \n",
       "4  Donald Trump heads for Scotland to reopen a go...   \n",
       "\n",
       "                                                text       subject  \\\n",
       "0  21st Century Wire says Ben Stein, reputable pr...       US_News   \n",
       "1  WASHINGTON (Reuters) - U.S. President Donald T...  politicsNews   \n",
       "2  (Reuters) - Puerto Rico Governor Ricardo Rosse...  politicsNews   \n",
       "3  On Monday, Donald Trump once again embarrassed...          News   \n",
       "4  GLASGOW, Scotland (Reuters) - Most U.S. presid...  politicsNews   \n",
       "\n",
       "                  date label  \\\n",
       "0    February 13, 2017  FAKE   \n",
       "1       April 5, 2017   TRUE   \n",
       "2  September 27, 2017   TRUE   \n",
       "3         May 22, 2017  FAKE   \n",
       "4       June 24, 2016   TRUE   \n",
       "\n",
       "                                         title_clean  \\\n",
       "0  ben stein call th circuit court committed coup...   \n",
       "1  trump drop steve bannon national security council   \n",
       "2  puerto rico expects u lift jones act shipping ...   \n",
       "3  oops trump accidentally confirmed leaked israe...   \n",
       "4      donald trump head scotland reopen golf resort   \n",
       "\n",
       "                                          text_clean  \\\n",
       "0  st century wire say ben stein reputable profes...   \n",
       "1  washington reuters u president donald trump re...   \n",
       "2  reuters puerto rico governor ricardo rossello ...   \n",
       "3  monday donald trump embarrassed country accide...   \n",
       "4  glasgow scotland reuters u presidential candid...   \n",
       "\n",
       "                                       combined_text  \n",
       "0  ben stein call th circuit court committed coup...  \n",
       "1  trump drop steve bannon national security coun...  \n",
       "2  puerto rico expects u lift jones act shipping ...  \n",
       "3  oops trump accidentally confirmed leaked israe...  \n",
       "4  donald trump head scotland reopen golf resort ...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5dd9b8e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title            0\n",
       "text             0\n",
       "subject          0\n",
       "date             0\n",
       "label            0\n",
       "title_clean      0\n",
       "text_clean       0\n",
       "combined_text    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed6e98b",
   "metadata": {},
   "source": [
    "## **Chapter 4. Modelling & Evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef03d056",
   "metadata": {},
   "source": [
    "### **4.1 Stratified Train Test Split**\n",
    "\n",
    "We create a function to do two Stratified Train Test Splits to our data ensruing that we have 10% in both the validation split and the test split.\n",
    "\n",
    "A Stratified Split ensures we maintain the ratio of classes `Main/Fake` throughhout our splits\n",
    "\n",
    "We export the splits into csv files for GPU training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2e3e5a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 35344, 'val': 4419, 'test': 4419}\n",
      "Files saved succesfully under data/\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "def train_val_test_split_stratified(df, label_col=\"label\", test_size=0.1, val_size=0.1, seed=42):\n",
    "    y = df[label_col].values  # these are \"FAKE\"/\"TRUE\" strings\n",
    "\n",
    "    # First split: train+val vs test\n",
    "    sss1 = StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=seed)\n",
    "    idx_trainval, idx_test = next(sss1.split(df, y))\n",
    "    df_trainval = df.iloc[idx_trainval].reset_index(drop=True)\n",
    "    df_test = df.iloc[idx_test].reset_index(drop=True)\n",
    "\n",
    "    # Second split: train vs val\n",
    "    sss2 = StratifiedShuffleSplit(n_splits=1, test_size=val_size/(1-test_size), random_state=seed)\n",
    "    y_tv = df_trainval[label_col].values\n",
    "    idx_train, idx_val = next(sss2.split(df_trainval, y_tv))\n",
    "    df_train = df_trainval.iloc[idx_train].reset_index(drop=True)\n",
    "    df_val = df_trainval.iloc[idx_val].reset_index(drop=True)\n",
    "\n",
    "    return df_train, df_val, df_test\n",
    "\n",
    "# Run the split\n",
    "df_train, df_val, df_test = train_val_test_split_stratified(\n",
    "    df, label_col=\"label\", test_size=0.10, val_size=0.10, seed=42\n",
    ")\n",
    "\n",
    "# Print sizes\n",
    "print({k: len(v) for k,v in {\"train\": df_train, \"val\": df_val, \"test\": df_test}.items()})\n",
    "\n",
    "# Save the raw splits\n",
    "from pathlib import Path\n",
    "split_dir = Path(\"../data\")        # define as Path, not string\n",
    "split_dir.mkdir(parents=True, exist_ok=True)  # make sure folder exists\n",
    "\n",
    "df_train.to_csv(split_dir / \"train.csv\", index=False)\n",
    "df_val.to_csv(split_dir / \"val.csv\", index=False)\n",
    "df_test.to_csv(split_dir / \"test.csv\", index=False)\n",
    "print(\"Files saved succesfully under data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d1ff1d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"../data/train.csv\")\n",
    "df_val   = pd.read_csv(\"../data/val.csv\")\n",
    "df_test  = pd.read_csv(\"../data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3f82baad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title            0\n",
       "text             0\n",
       "subject          0\n",
       "date             0\n",
       "label            0\n",
       "title_clean      0\n",
       "text_clean       0\n",
       "combined_text    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1421279d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(split_dir / \"full.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b1c603",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
