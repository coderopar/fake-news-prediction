{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e33deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import zipfile\n",
    "from wordcloud import WordCloud\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download resources once\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"punkt_tab\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "nltk.download(\"stopwords\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4ecffc",
   "metadata": {},
   "source": [
    "### **2.2 Load and Preview Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd1651a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile(\"../data/archive.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"unzipped_data\")\n",
    "    \n",
    "print(\"Files extracted successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce84aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(\"unzipped_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fe22ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_df = pd.read_csv(\"unzipped_data/Fake.csv\")\n",
    "true_df = pd.read_csv(\"unzipped_data/True.csv\")\n",
    "\n",
    "print(\"Fake News Dataset:\", fake_df.shape)\n",
    "print(\"True News Dataset:\", true_df.shape)\n",
    "\n",
    "fake_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09523c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge and label\n",
    "\n",
    "#Add a label column\n",
    "fake_df[\"label\"] = \"FAKE\"\n",
    "true_df[\"label\"] = \"TRUE\"\n",
    "\n",
    "#Merge into one dataset\n",
    "df = pd.concat([fake_df, true_df], ignore_index = True)\n",
    "\n",
    "#Shuffle the rows so FAKE and TRUE are mixed\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "#Check the structure\n",
    "print(df.shape)\n",
    "print(df[\"label\"].value_counts())\n",
    "print(df.info())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e382ebb",
   "metadata": {},
   "source": [
    "## **Chapter 3. Data Preparation**\n",
    "In this section, we will "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836a0a9d",
   "metadata": {},
   "source": [
    "### **3.1 Lowercasing & URL removal**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eee104a",
   "metadata": {},
   "source": [
    "**1. Defining Preprocessing Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c825954f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text_lowercase_url(text):\n",
    "    \"\"\"\n",
    "    MAIN PREPROCESSING FUNCTION:\n",
    "    - Converts text to lowercase\n",
    "    - Removes URLs, hyperlinks, and website addresses\n",
    "    - Handles missing values safely\n",
    "    - Cleans extra whitespace\n",
    "    \"\"\"\n",
    "    # Handle missing values\n",
    "    if pd.isna(text) or text is None:\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to string to ensure consistent processing\n",
    "    text = str(text)\n",
    "    \n",
    "    # COMPREHENSIVE URL REMOVAL PATTERN:\n",
    "    url_pattern = r'https?://\\S+|www\\.\\S+|\\S+\\.(com|org|net|edu|gov|io|co|uk)\\S*|bit\\.ly/\\S+|t\\.co/\\S+'\n",
    "    \n",
    "    # Remove all URLs from text\n",
    "    text = re.sub(url_pattern, '', text)\n",
    "    \n",
    "    # Convert entire text to lowercase for consistency\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Clean up extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "print(\"Preprocessing functions defined!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b50db3",
   "metadata": {},
   "source": [
    "**2. Quality check functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36a6a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_url(text):\n",
    "    \"\"\"Check if text contains any URLs\"\"\"\n",
    "    url_pattern = r'https?://|www\\.|\\.[a-z]{2,}'\n",
    "    return bool(re.search(url_pattern, str(text).lower()))\n",
    "\n",
    "def count_uppercase(text):\n",
    "    \"\"\"Count uppercase characters in text\"\"\"\n",
    "    return sum(1 for char in str(text) if char.isupper())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb9e716",
   "metadata": {},
   "source": [
    "### **3.2 Remove Non-Alphabetic Characters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f82acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_RE   = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "HTML_RE  = re.compile(r'<.*?>')\n",
    "NONALPH  = re.compile(r'[^a-z\\s]+')     # keep letters & spaces only\n",
    "WS_RE    = re.compile(r'\\s+')\n",
    "\n",
    "# Defining Preprocessing Function\n",
    "def _keep_alpha_only(text: str) -> str:\n",
    "    text = NONALPH.sub(\" \", text)    # remove non-letters\n",
    "    text = WS_RE.sub(\" \", text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da527af",
   "metadata": {},
   "source": [
    "### **3.3 Lemmatization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32faae38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lemmatizer and stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Define preprocessing + lemmatization function\n",
    "def preprocess_and_lemmatize(text):\n",
    "    if isinstance(text, str):  # make sure it's a string\n",
    "        # Lowercase\n",
    "        text = text.lower()\n",
    "\n",
    "        # Remove punctuation, numbers, special chars\n",
    "        text = re.sub(r'[^a-z\\s]', '', text)\n",
    "\n",
    "        # Tokenize\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "\n",
    "        # Remove stopwords + lemmatize\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "\n",
    "        return \" \".join(tokens)\n",
    "    else:\n",
    "        return \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e6a4d2",
   "metadata": {},
   "source": [
    "### **3.4 Apply Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621c251c",
   "metadata": {},
   "source": [
    "**Defining Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec713ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining function to apply preprocessing\n",
    "def apply_preprocessing(text: str) -> str:\n",
    "    \"\"\"\n",
    "      1) preprocess_text_lowercase_url  [lowercase + URL removal + whitespace clean]\n",
    "      2) _keep_alpha_only                [remove non-alphabetic, collapse spaces]\n",
    "      3) preprocess_and_lemmatize [tokenize, drop stopwords, lemmatize]\n",
    "    \"\"\"\n",
    "    # Step 1 (Teammate 4)\n",
    "    text = preprocess_text_lowercase_url(text)\n",
    "\n",
    "    # Optional (if HTML present in some sources): strip simple tags BEFORE alpha-only\n",
    "    # text = HTML_RE.sub(\" \", text)  # Uncomment if needed\n",
    "\n",
    "    # Step 2 (Teammate 4)\n",
    "    text = _keep_alpha_only(text)\n",
    "\n",
    "    # Step 3 (Teammate 5, adapter)\n",
    "    text = preprocess_and_lemmatize(text)\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c5e64b",
   "metadata": {},
   "source": [
    "**Applying Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9283bdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['title_clean'] = df['title'].apply(apply_preprocessing)\n",
    "df['text_clean'] = df['text'].apply(apply_preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b3c176",
   "metadata": {},
   "source": [
    "### **3.5 Preprocessing Tests**\n",
    "\n",
    "These tests validate that our cleaned columns (`title_clean`, `text_clean`) are\n",
    "correctly produced from the raw text (`title`, `text`) using the\n",
    "`apply_preprocessing` pipeline.\n",
    "\n",
    "### What the tests check\n",
    "1. **Column existence & type**  \n",
    "   - Both `*_clean` columns exist, are pandas Series, and contain strings.\n",
    "\n",
    "2. **Basic cleaning guarantees**  \n",
    "   - No URLs remain in the text.  \n",
    "   - All text is lowercase.  \n",
    "   - Only alphabetic characters and spaces are present.  \n",
    "   - No leading, trailing, or repeated whitespace.  \n",
    "   - No `NaN` values; all entries are strings.\n",
    "\n",
    "3. **Pipeline correctness**  \n",
    "   - Each `*_clean` column equals exactly one pass of `apply_preprocessing` on the raw column.\n",
    "\n",
    "4. **Non-empty output for meaningful inputs**  \n",
    "   - If a raw row has meaningful alphabetic content (not just URLs, HTML, or stopwords),\n",
    "     the cleaned version is not empty.\n",
    "\n",
    "5. **Length sanity**  \n",
    "   - Cleaned text is usually shorter or equal in length compared to raw text.  \n",
    "   - The majority of meaningful rows remain non-empty after preprocessing.\n",
    "\n",
    "6. **Summary statistics**  \n",
    "   - Reports row counts, percentage of non-empty values, median lengths,\n",
    "     and how often cleaned length ≤ raw length.\n",
    "\n",
    "### Why these tests matter\n",
    "They ensure our preprocessing pipeline:\n",
    "- Safely handles edge cases (URLs, HTML, missing values).  \n",
    "- Produces consistent, normalized text suitable for tokenization.  \n",
    "- Doesn’t over-clean and erase meaningful content.  \n",
    "- Stays aligned with the defined stopword/lemmatization rules.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a4222f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Neutralize any previously-defined idempotence checker lingering in memory ---\n",
    "def _assert_idempotent(*args, **kwargs):\n",
    "    return  # disabled by design\n",
    "\n",
    "# --- Patterns ---\n",
    "ALPHA_SPACE_RE = re.compile(r\"^[a-z ]*$\")  # letters + spaces only\n",
    "URL_RE = re.compile(r\"(https?://\\S+|www\\.\\S+|\\S+\\.(com|org|net|edu|gov|io|co|uk)\\S*|bit\\.ly/\\S+|t\\.co/\\S+)\")\n",
    "\n",
    "# --- Helpers ---\n",
    "def _example(series, mask):\n",
    "    m = mask.values if hasattr(mask, \"values\") else mask\n",
    "    idx = np.flatnonzero(m)\n",
    "    if len(idx):\n",
    "        i = idx[0]\n",
    "        try:\n",
    "            return str(series.iloc[i])[:200]\n",
    "        except Exception:\n",
    "            return \"<unavailable>\"\n",
    "    return \"<none>\"\n",
    "\n",
    "def _assert_series_exists_and_string(s: pd.Series, name: str):\n",
    "    assert isinstance(s, pd.Series), f\"{name} is not a pandas Series.\"\n",
    "    assert s.dtype == \"object\" or pd.api.types.is_string_dtype(s), f\"{name} must be string-like dtype.\"\n",
    "\n",
    "def _assert_no_urls(s: pd.Series, name: str):\n",
    "    mask = s.fillna(\"\").str.contains(URL_RE)\n",
    "    assert not mask.any(), f\"{name}: URLs found. Example: { _example(s, mask)!r }\"\n",
    "\n",
    "def _assert_lowercase_only(s: pd.Series, name: str):\n",
    "    mask = s.fillna(\"\").str.contains(r\"[A-Z]\")\n",
    "    assert not mask.any(), f\"{name}: Uppercase letters found. Example: { _example(s, mask)!r }\"\n",
    "\n",
    "def _assert_alpha_space_only(s: pd.Series, name: str):\n",
    "    mask = s.fillna(\"\").apply(lambda x: bool(x) and ALPHA_SPACE_RE.match(x) is None)\n",
    "    assert not mask.any(), f\"{name}: Non alpha/space characters found. Example: { _example(s, mask)!r }\"\n",
    "\n",
    "def _assert_no_extra_whitespace(s: pd.Series, name: str):\n",
    "    s2 = s.fillna(\"\")\n",
    "    leading = s2.str.match(r\"^\\s\")\n",
    "    trailing = s2.str.contains(r\"\\s$\")\n",
    "    doubles  = s2.str.contains(r\"\\s{2,}\")\n",
    "    assert not leading.any(),  f\"{name}: Leading spaces found. Example: { _example(s, leading)!r }\"\n",
    "    assert not trailing.any(), f\"{name}: Trailing spaces found. Example: { _example(s, trailing)!r }\"\n",
    "    assert not doubles.any(),  f\"{name}: Multiple consecutive spaces found. Example: { _example(s, doubles)!r }\"\n",
    "\n",
    "def _assert_no_nans_and_strings(s: pd.Series, name: str):\n",
    "    assert not s.isna().any(), f\"{name}: NaNs present.\"\n",
    "    assert s.map(lambda x: isinstance(x, str)).all(), f\"{name}: Non-string detected.\"\n",
    "\n",
    "def _assert_matches_single_pass(raw: pd.Series, cleaned: pd.Series, name: str, fn):\n",
    "    recomputed = raw.apply(fn)\n",
    "    diff = cleaned != recomputed\n",
    "    if diff.any():\n",
    "        i = np.flatnonzero(diff.values)[0]\n",
    "        raise AssertionError(\n",
    "            f\"{name}: cleaned column != single-pass pipeline.\\n\"\n",
    "            f\"  raw:                       {raw.iloc[i]!r}\\n\"\n",
    "            f\"  cleaned (your column):     {cleaned.iloc[i]!r}\\n\"\n",
    "            f\"  recomputed({fn.__name__}): {recomputed.iloc[i]!r}\"\n",
    "        )\n",
    "\n",
    "# --- Meaningfulness heuristic aligned with your pipeline's stopwords ---\n",
    "# Prefer your NLTK stopword set if present; otherwise use a richer fallback.\n",
    "try:\n",
    "    STOPWORDS_FOR_TEST = set(stop_words)  # uses your actual pipeline's stopwords if defined\n",
    "    if not isinstance(STOPWORDS_FOR_TEST, set):\n",
    "        STOPWORDS_FOR_TEST = set(STOPWORDS_FOR_TEST)\n",
    "except NameError:\n",
    "    STOPWORDS_FOR_TEST = set(\"\"\"\n",
    "    a an the and or but if then else when while is are was were be been being am\n",
    "    to of in on at by for from as that this it its into over under about with\n",
    "    i you he she we they me my mine your yours his her hers our ours their theirs\n",
    "    do does did doing done have has had having be been being not no nor\n",
    "    s t d ll re ve m y\n",
    "    \"\"\".split())\n",
    "\n",
    "def _raw_has_meaningful_letters(s: str) -> bool:\n",
    "    \"\"\"Decide if raw text *should* yield non-empty output after your pipeline.\n",
    "       Mirrors your cleaning (strip URLs/HTML/non-alpha) and uses your stopwords.\n",
    "    \"\"\"\n",
    "    if s is None:\n",
    "        return False\n",
    "    s = re.sub(URL_RE, \" \", str(s))          # strip URLs\n",
    "    s = re.sub(r\"<.*?>\", \" \", s)             # strip simple HTML tags\n",
    "    s = re.sub(r\"[^A-Za-z\\s]+\", \" \", s)      # keep letters + spaces\n",
    "    toks = [t.lower() for t in s.split()]\n",
    "    toks = [t for t in toks if t not in STOPWORDS_FOR_TEST]\n",
    "    return any(len(t) >= 2 for t in toks)\n",
    "\n",
    "def _assert_non_empty_when_input_had_letters(raw: pd.Series, cleaned: pd.Series, name: str):\n",
    "    # Only rows that truly have content beyond your stopwords must remain non-empty.\n",
    "    meaningful = raw.fillna(\"\").map(_raw_has_meaningful_letters)\n",
    "    empties = cleaned == \"\"\n",
    "    bad = meaningful & empties\n",
    "    assert not bad.any(), (\n",
    "        f\"{name}: Became empty despite meaningful alphabetic input. \"\n",
    "        f\"Example raw: { _example(raw, bad)!r }\"\n",
    "    )\n",
    "\n",
    "def _length_sanity(raw: pd.Series, cleaned: pd.Series, name: str, min_non_empty_ratio=0.8):\n",
    "    mask_meaningful = raw.fillna(\"\").map(_raw_has_meaningful_letters)\n",
    "    rl = raw[mask_meaningful].fillna(\"\").str.len()\n",
    "    cl = cleaned[mask_meaningful].fillna(\"\").str.len()\n",
    "    if len(rl) == 0:\n",
    "        return\n",
    "    assert (cl <= rl).mean() >= 0.5, f\"{name}: Too many cleaned rows longer than original (meaningful subset).\"\n",
    "    ratio_nonempty = (cl > 0).mean()\n",
    "    assert ratio_nonempty >= min_non_empty_ratio, (\n",
    "        f\"{name}: Too many empty cleaned rows among meaningful inputs \"\n",
    "        f\"(ratio_nonempty={ratio_nonempty:.2f}).\"\n",
    "    )\n",
    "\n",
    "# --- Run checks on both columns ---\n",
    "for raw_col, clean_col in [(\"title\", \"title_clean\"), (\"text\", \"text_clean\")]:\n",
    "    assert raw_col in df.columns,   f\"Missing original column: {raw_col}\"\n",
    "    assert clean_col in df.columns, f\"Missing cleaned column: {clean_col}\"\n",
    "\n",
    "    s_raw = df[raw_col]\n",
    "    s_cln = df[clean_col]\n",
    "\n",
    "    _assert_series_exists_and_string(s_cln, clean_col)\n",
    "    _assert_no_urls(s_cln, clean_col)\n",
    "    _assert_lowercase_only(s_cln, clean_col)\n",
    "    _assert_alpha_space_only(s_cln, clean_col)\n",
    "    _assert_no_extra_whitespace(s_cln, clean_col)\n",
    "    _assert_no_nans_and_strings(s_cln, clean_col)\n",
    "\n",
    "    # Authoritative: cleaned equals ONE single pass over raw\n",
    "    _assert_matches_single_pass(s_raw, s_cln, clean_col, apply_preprocessing)\n",
    "\n",
    "    # Heuristic-based expectations (aligned with your stopwords)\n",
    "    _assert_non_empty_when_input_had_letters(s_raw, s_cln, clean_col)\n",
    "    _length_sanity(s_raw, s_cln, clean_col, min_non_empty_ratio=0.8)\n",
    "\n",
    "# --- Brief summary ---\n",
    "def _summary_block(name: str, raw: pd.Series, clean: pd.Series):\n",
    "    rl = raw.fillna(\"\").str.len()\n",
    "    cl = clean.fillna(\"\").str.len()\n",
    "    print(f\"\\n[{name}] rows={len(raw)}\")\n",
    "    print(f\"  Non-empty (raw/clean): {(rl>0).mean():.2%} / {(cl>0).mean():.2%}\")\n",
    "    print(f\"  Median length (raw/clean): {int(rl.median() if len(rl) else 0)} / {int(cl.median() if len(cl) else 0)}\")\n",
    "    print(f\"  <= length preserved ratio: {(cl<=rl).mean():.2%}\")\n",
    "\n",
    "_summary_block(\"TITLE\", df[\"title\"], df[\"title_clean\"])\n",
    "_summary_block(\"TEXT\",  df[\"text\"],  df[\"text_clean\"])\n",
    "\n",
    "print(\"\\n All dataframe preprocessing checks passed (single-pass contract; stopword-aware meaningfulness).\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
