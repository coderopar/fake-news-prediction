{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be17e1e9",
   "metadata": {},
   "source": [
    "### **Preprocessing Tests**\n",
    "These tests validate that our cleaned columns (`title_clean`, `text_clean`) are\n",
    "correctly produced from the raw text (`title`, `text`) using the\n",
    "`apply_preprocessing` pipeline.\n",
    "\n",
    "### What the tests check\n",
    "1. **Column existence & type**  \n",
    "   - Both `*_clean` columns exist, are pandas Series, and contain strings.\n",
    "\n",
    "2. **Basic cleaning guarantees**  \n",
    "   - No URLs remain in the text.  \n",
    "   - All text is lowercase.  \n",
    "   - Only alphabetic characters and spaces are present.  \n",
    "   - No leading, trailing, or repeated whitespace.  \n",
    "   - No `NaN` values; all entries are strings.\n",
    "\n",
    "3. **Pipeline correctness**  \n",
    "   - Each `*_clean` column equals exactly one pass of `apply_preprocessing` on the raw column.\n",
    "\n",
    "4. **Non-empty output for meaningful inputs**  \n",
    "   - If a raw row has meaningful alphabetic content (not just URLs, HTML, or stopwords),\n",
    "     the cleaned version is not empty.\n",
    "\n",
    "5. **Length sanity**  \n",
    "   - Cleaned text is usually shorter or equal in length compared to raw text.  \n",
    "   - The majority of meaningful rows remain non-empty after preprocessing.\n",
    "\n",
    "6. **Summary statistics**  \n",
    "   - Reports row counts, percentage of non-empty values, median lengths,\n",
    "     and how often cleaned length ≤ raw length.\n",
    "\n",
    "### Why these tests matter\n",
    "They ensure our preprocessing pipeline:\n",
    "- Safely handles edge cases (URLs, HTML, missing values).  \n",
    "- Produces consistent, normalized text suitable for tokenization.  \n",
    "- Doesn’t over-clean and erase meaningful content.  \n",
    "- Stays aligned with the defined stopword/lemmatization rules.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6127ea95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Neutralize any previously-defined idempotence checker lingering in memory ---\n",
    "def _assert_idempotent(*args, **kwargs):\n",
    "    return  # disabled by design\n",
    "\n",
    "# --- Patterns ---\n",
    "ALPHA_SPACE_RE = re.compile(r\"^[a-z ]*$\")  # letters + spaces only\n",
    "URL_RE = re.compile(r\"(https?://\\S+|www\\.\\S+|\\S+\\.(com|org|net|edu|gov|io|co|uk)\\S*|bit\\.ly/\\S+|t\\.co/\\S+)\")\n",
    "\n",
    "# --- Helpers ---\n",
    "def _example(series, mask):\n",
    "    m = mask.values if hasattr(mask, \"values\") else mask\n",
    "    idx = np.flatnonzero(m)\n",
    "    if len(idx):\n",
    "        i = idx[0]\n",
    "        try:\n",
    "            return str(series.iloc[i])[:200]\n",
    "        except Exception:\n",
    "            return \"<unavailable>\"\n",
    "    return \"<none>\"\n",
    "\n",
    "def _assert_series_exists_and_string(s: pd.Series, name: str):\n",
    "    assert isinstance(s, pd.Series), f\"{name} is not a pandas Series.\"\n",
    "    assert s.dtype == \"object\" or pd.api.types.is_string_dtype(s), f\"{name} must be string-like dtype.\"\n",
    "\n",
    "def _assert_no_urls(s: pd.Series, name: str):\n",
    "    mask = s.fillna(\"\").str.contains(URL_RE)\n",
    "    assert not mask.any(), f\"{name}: URLs found. Example: { _example(s, mask)!r }\"\n",
    "\n",
    "def _assert_lowercase_only(s: pd.Series, name: str):\n",
    "    mask = s.fillna(\"\").str.contains(r\"[A-Z]\")\n",
    "    assert not mask.any(), f\"{name}: Uppercase letters found. Example: { _example(s, mask)!r }\"\n",
    "\n",
    "def _assert_alpha_space_only(s: pd.Series, name: str):\n",
    "    mask = s.fillna(\"\").apply(lambda x: bool(x) and ALPHA_SPACE_RE.match(x) is None)\n",
    "    assert not mask.any(), f\"{name}: Non alpha/space characters found. Example: { _example(s, mask)!r }\"\n",
    "\n",
    "def _assert_no_extra_whitespace(s: pd.Series, name: str):\n",
    "    s2 = s.fillna(\"\")\n",
    "    leading = s2.str.match(r\"^\\s\")\n",
    "    trailing = s2.str.contains(r\"\\s$\")\n",
    "    doubles  = s2.str.contains(r\"\\s{2,}\")\n",
    "    assert not leading.any(),  f\"{name}: Leading spaces found. Example: { _example(s, leading)!r }\"\n",
    "    assert not trailing.any(), f\"{name}: Trailing spaces found. Example: { _example(s, trailing)!r }\"\n",
    "    assert not doubles.any(),  f\"{name}: Multiple consecutive spaces found. Example: { _example(s, doubles)!r }\"\n",
    "\n",
    "def _assert_no_nans_and_strings(s: pd.Series, name: str):\n",
    "    assert not s.isna().any(), f\"{name}: NaNs present.\"\n",
    "    assert s.map(lambda x: isinstance(x, str)).all(), f\"{name}: Non-string detected.\"\n",
    "\n",
    "def _assert_matches_single_pass(raw: pd.Series, cleaned: pd.Series, name: str, fn):\n",
    "    recomputed = raw.apply(fn)\n",
    "    diff = cleaned != recomputed\n",
    "    if diff.any():\n",
    "        i = np.flatnonzero(diff.values)[0]\n",
    "        raise AssertionError(\n",
    "            f\"{name}: cleaned column != single-pass pipeline.\\n\"\n",
    "            f\"  raw:                       {raw.iloc[i]!r}\\n\"\n",
    "            f\"  cleaned (your column):     {cleaned.iloc[i]!r}\\n\"\n",
    "            f\"  recomputed({fn.__name__}): {recomputed.iloc[i]!r}\"\n",
    "        )\n",
    "\n",
    "# --- Meaningfulness heuristic aligned with your pipeline's stopwords ---\n",
    "# Prefer your NLTK stopword set if present; otherwise use a richer fallback.\n",
    "try:\n",
    "    STOPWORDS_FOR_TEST = set(stop_words)  # uses your actual pipeline's stopwords if defined\n",
    "    if not isinstance(STOPWORDS_FOR_TEST, set):\n",
    "        STOPWORDS_FOR_TEST = set(STOPWORDS_FOR_TEST)\n",
    "except NameError:\n",
    "    STOPWORDS_FOR_TEST = set(\"\"\"\n",
    "    a an the and or but if then else when while is are was were be been being am\n",
    "    to of in on at by for from as that this it its into over under about with\n",
    "    i you he she we they me my mine your yours his her hers our ours their theirs\n",
    "    do does did doing done have has had having be been being not no nor\n",
    "    s t d ll re ve m y\n",
    "    \"\"\".split())\n",
    "\n",
    "def _raw_has_meaningful_letters(s: str) -> bool:\n",
    "    \"\"\"Decide if raw text *should* yield non-empty output after your pipeline.\n",
    "       Mirrors your cleaning (strip URLs/HTML/non-alpha) and uses your stopwords.\n",
    "    \"\"\"\n",
    "    if s is None:\n",
    "        return False\n",
    "    s = re.sub(URL_RE, \" \", str(s))          # strip URLs\n",
    "    s = re.sub(r\"<.*?>\", \" \", s)             # strip simple HTML tags\n",
    "    s = re.sub(r\"[^A-Za-z\\s]+\", \" \", s)      # keep letters + spaces\n",
    "    toks = [t.lower() for t in s.split()]\n",
    "    toks = [t for t in toks if t not in STOPWORDS_FOR_TEST]\n",
    "    return any(len(t) >= 2 for t in toks)\n",
    "\n",
    "def _assert_non_empty_when_input_had_letters(raw: pd.Series, cleaned: pd.Series, name: str):\n",
    "    # Only rows that truly have content beyond your stopwords must remain non-empty.\n",
    "    meaningful = raw.fillna(\"\").map(_raw_has_meaningful_letters)\n",
    "    empties = cleaned == \"\"\n",
    "    bad = meaningful & empties\n",
    "    assert not bad.any(), (\n",
    "        f\"{name}: Became empty despite meaningful alphabetic input. \"\n",
    "        f\"Example raw: { _example(raw, bad)!r }\"\n",
    "    )\n",
    "\n",
    "def _length_sanity(raw: pd.Series, cleaned: pd.Series, name: str, min_non_empty_ratio=0.8):\n",
    "    mask_meaningful = raw.fillna(\"\").map(_raw_has_meaningful_letters)\n",
    "    rl = raw[mask_meaningful].fillna(\"\").str.len()\n",
    "    cl = cleaned[mask_meaningful].fillna(\"\").str.len()\n",
    "    if len(rl) == 0:\n",
    "        return\n",
    "    assert (cl <= rl).mean() >= 0.5, f\"{name}: Too many cleaned rows longer than original (meaningful subset).\"\n",
    "    ratio_nonempty = (cl > 0).mean()\n",
    "    assert ratio_nonempty >= min_non_empty_ratio, (\n",
    "        f\"{name}: Too many empty cleaned rows among meaningful inputs \"\n",
    "        f\"(ratio_nonempty={ratio_nonempty:.2f}).\"\n",
    "    )\n",
    "\n",
    "# --- Run checks on both columns ---\n",
    "for raw_col, clean_col in [(\"title\", \"title_clean\"), (\"text\", \"text_clean\")]:\n",
    "    assert raw_col in df.columns,   f\"Missing original column: {raw_col}\"\n",
    "    assert clean_col in df.columns, f\"Missing cleaned column: {clean_col}\"\n",
    "\n",
    "    s_raw = df[raw_col]\n",
    "    s_cln = df[clean_col]\n",
    "\n",
    "    _assert_series_exists_and_string(s_cln, clean_col)\n",
    "    _assert_no_urls(s_cln, clean_col)\n",
    "    _assert_lowercase_only(s_cln, clean_col)\n",
    "    _assert_alpha_space_only(s_cln, clean_col)\n",
    "    _assert_no_extra_whitespace(s_cln, clean_col)\n",
    "    _assert_no_nans_and_strings(s_cln, clean_col)\n",
    "\n",
    "    # Authoritative: cleaned equals ONE single pass over raw\n",
    "    _assert_matches_single_pass(s_raw, s_cln, clean_col, apply_preprocessing)\n",
    "\n",
    "    # Heuristic-based expectations (aligned with your stopwords)\n",
    "    _assert_non_empty_when_input_had_letters(s_raw, s_cln, clean_col)\n",
    "    _length_sanity(s_raw, s_cln, clean_col, min_non_empty_ratio=0.8)\n",
    "\n",
    "# --- Brief summary ---\n",
    "def _summary_block(name: str, raw: pd.Series, clean: pd.Series):\n",
    "    rl = raw.fillna(\"\").str.len()\n",
    "    cl = clean.fillna(\"\").str.len()\n",
    "    print(f\"\\n[{name}] rows={len(raw)}\")\n",
    "    print(f\"  Non-empty (raw/clean): {(rl>0).mean():.2%} / {(cl>0).mean():.2%}\")\n",
    "    print(f\"  Median length (raw/clean): {int(rl.median() if len(rl) else 0)} / {int(cl.median() if len(cl) else 0)}\")\n",
    "    print(f\"  <= length preserved ratio: {(cl<=rl).mean():.2%}\")\n",
    "\n",
    "_summary_block(\"TITLE\", df[\"title\"], df[\"title_clean\"])\n",
    "_summary_block(\"TEXT\",  df[\"text\"],  df[\"text_clean\"])\n",
    "\n",
    "print(\"\\n All dataframe preprocessing checks passed (single-pass contract; stopword-aware meaningfulness).\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
