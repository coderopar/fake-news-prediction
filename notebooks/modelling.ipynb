{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e33deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import zipfile\n",
    "from wordcloud import WordCloud\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# Download resources once\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"punkt_tab\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "nltk.download(\"stopwords\")\n",
    "# Core Python / Utilities\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import torch\n",
    "import sys\n",
    "import inspect\n",
    "\n",
    "# Data Handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# NLP / Text Preprocessing\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Scikit-learn (ML, metrics, preprocessing)\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_recall_fscore_support,\n",
    "    roc_auc_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    "    roc_curve\n",
    ")\n",
    "import joblib\n",
    "\n",
    "# Deep Learning (LSTM)\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import (\n",
    "    Embedding,\n",
    "    LSTM,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    SpatialDropout1D,\n",
    "    Bidirectional\n",
    ")\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Transformers (BERT)\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW  \n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from transformers import BertModel\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, get_linear_schedule_with_warmup\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "# Utility for progress\n",
    "from tqdm import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4ecffc",
   "metadata": {},
   "source": [
    "### **2.2 Load and Preview Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcd1651a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files extracted successfully!\n"
     ]
    }
   ],
   "source": [
    "with zipfile.ZipFile(\"../data/archive.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"unzipped_data\")\n",
    "    \n",
    "print(\"Files extracted successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ce84aa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Fake.csv', 'True.csv']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(\"unzipped_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63fe22ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fake News Dataset: (23481, 4)\n",
      "True News Dataset: (21417, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Donald Trump Sends Out Embarrassing New Year’...</td>\n",
       "      <td>Donald Trump just couldn t wish all Americans ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Drunk Bragging Trump Staffer Started Russian ...</td>\n",
       "      <td>House Intelligence Committee Chairman Devin Nu...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sheriff David Clarke Becomes An Internet Joke...</td>\n",
       "      <td>On Friday, it was revealed that former Milwauk...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 30, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trump Is So Obsessed He Even Has Obama’s Name...</td>\n",
       "      <td>On Christmas day, Donald Trump announced that ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 29, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pope Francis Just Called Out Donald Trump Dur...</td>\n",
       "      <td>Pope Francis used his annual Christmas Day mes...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 25, 2017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0   Donald Trump Sends Out Embarrassing New Year’...   \n",
       "1   Drunk Bragging Trump Staffer Started Russian ...   \n",
       "2   Sheriff David Clarke Becomes An Internet Joke...   \n",
       "3   Trump Is So Obsessed He Even Has Obama’s Name...   \n",
       "4   Pope Francis Just Called Out Donald Trump Dur...   \n",
       "\n",
       "                                                text subject  \\\n",
       "0  Donald Trump just couldn t wish all Americans ...    News   \n",
       "1  House Intelligence Committee Chairman Devin Nu...    News   \n",
       "2  On Friday, it was revealed that former Milwauk...    News   \n",
       "3  On Christmas day, Donald Trump announced that ...    News   \n",
       "4  Pope Francis used his annual Christmas Day mes...    News   \n",
       "\n",
       "                date  \n",
       "0  December 31, 2017  \n",
       "1  December 31, 2017  \n",
       "2  December 30, 2017  \n",
       "3  December 29, 2017  \n",
       "4  December 25, 2017  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_df = pd.read_csv(\"unzipped_data/Fake.csv\")\n",
    "true_df = pd.read_csv(\"unzipped_data/True.csv\")\n",
    "\n",
    "print(\"Fake News Dataset:\", fake_df.shape)\n",
    "print(\"True News Dataset:\", true_df.shape)\n",
    "\n",
    "fake_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09523c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44898, 5)\n",
      "label\n",
      "FAKE    23481\n",
      "TRUE    21417\n",
      "Name: count, dtype: int64\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 44898 entries, 0 to 44897\n",
      "Data columns (total 5 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   title    44898 non-null  object\n",
      " 1   text     44898 non-null  object\n",
      " 2   subject  44898 non-null  object\n",
      " 3   date     44898 non-null  object\n",
      " 4   label    44898 non-null  object\n",
      "dtypes: object(5)\n",
      "memory usage: 1.7+ MB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ben Stein Calls Out 9th Circuit Court: Committ...</td>\n",
       "      <td>21st Century Wire says Ben Stein, reputable pr...</td>\n",
       "      <td>US_News</td>\n",
       "      <td>February 13, 2017</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Trump drops Steve Bannon from National Securit...</td>\n",
       "      <td>WASHINGTON (Reuters) - U.S. President Donald T...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>April 5, 2017</td>\n",
       "      <td>TRUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Puerto Rico expects U.S. to lift Jones Act shi...</td>\n",
       "      <td>(Reuters) - Puerto Rico Governor Ricardo Rosse...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>September 27, 2017</td>\n",
       "      <td>TRUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OOPS: Trump Just Accidentally Confirmed He Le...</td>\n",
       "      <td>On Monday, Donald Trump once again embarrassed...</td>\n",
       "      <td>News</td>\n",
       "      <td>May 22, 2017</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Donald Trump heads for Scotland to reopen a go...</td>\n",
       "      <td>GLASGOW, Scotland (Reuters) - Most U.S. presid...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>June 24, 2016</td>\n",
       "      <td>TRUE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Ben Stein Calls Out 9th Circuit Court: Committ...   \n",
       "1  Trump drops Steve Bannon from National Securit...   \n",
       "2  Puerto Rico expects U.S. to lift Jones Act shi...   \n",
       "3   OOPS: Trump Just Accidentally Confirmed He Le...   \n",
       "4  Donald Trump heads for Scotland to reopen a go...   \n",
       "\n",
       "                                                text       subject  \\\n",
       "0  21st Century Wire says Ben Stein, reputable pr...       US_News   \n",
       "1  WASHINGTON (Reuters) - U.S. President Donald T...  politicsNews   \n",
       "2  (Reuters) - Puerto Rico Governor Ricardo Rosse...  politicsNews   \n",
       "3  On Monday, Donald Trump once again embarrassed...          News   \n",
       "4  GLASGOW, Scotland (Reuters) - Most U.S. presid...  politicsNews   \n",
       "\n",
       "                  date label  \n",
       "0    February 13, 2017  FAKE  \n",
       "1       April 5, 2017   TRUE  \n",
       "2  September 27, 2017   TRUE  \n",
       "3         May 22, 2017  FAKE  \n",
       "4       June 24, 2016   TRUE  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#merge and label\n",
    "\n",
    "#Add a label column\n",
    "fake_df[\"label\"] = \"FAKE\"\n",
    "true_df[\"label\"] = \"TRUE\"\n",
    "\n",
    "#Merge into one dataset\n",
    "df = pd.concat([fake_df, true_df], ignore_index = True)\n",
    "\n",
    "#Shuffle the rows so FAKE and TRUE are mixed\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "#Check the structure\n",
    "print(df.shape)\n",
    "print(df[\"label\"].value_counts())\n",
    "print(df.info())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e382ebb",
   "metadata": {},
   "source": [
    "## **Chapter 3. Data Preparation**\n",
    "In this section, we will "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836a0a9d",
   "metadata": {},
   "source": [
    "### **3.1 Lowercasing & URL removal**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eee104a",
   "metadata": {},
   "source": [
    "**1. Defining Preprocessing Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c825954f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing functions defined!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text_lowercase_url(text):\n",
    "    \"\"\"\n",
    "    MAIN PREPROCESSING FUNCTION:\n",
    "    - Converts text to lowercase\n",
    "    - Removes URLs, hyperlinks, and website addresses\n",
    "    - Handles missing values safely\n",
    "    - Cleans extra whitespace\n",
    "    \"\"\"\n",
    "    # Handle missing values\n",
    "    if pd.isna(text) or text is None:\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to string to ensure consistent processing\n",
    "    text = str(text)\n",
    "    \n",
    "    # COMPREHENSIVE URL REMOVAL PATTERN:\n",
    "    url_pattern = r'https?://\\S+|www\\.\\S+|\\S+\\.(com|org|net|edu|gov|io|co|uk)\\S*|bit\\.ly/\\S+|t\\.co/\\S+'\n",
    "    \n",
    "    # Remove all URLs from text\n",
    "    text = re.sub(url_pattern, '', text)\n",
    "    \n",
    "    # Convert entire text to lowercase for consistency\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Clean up extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "print(\"Preprocessing functions defined!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b50db3",
   "metadata": {},
   "source": [
    "**2. Quality check functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d36a6a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_url(text):\n",
    "    \"\"\"Check if text contains any URLs\"\"\"\n",
    "    url_pattern = r'https?://|www\\.|\\.[a-z]{2,}'\n",
    "    return bool(re.search(url_pattern, str(text).lower()))\n",
    "\n",
    "def count_uppercase(text):\n",
    "    \"\"\"Count uppercase characters in text\"\"\"\n",
    "    return sum(1 for char in str(text) if char.isupper())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb9e716",
   "metadata": {},
   "source": [
    "### **3.2 Remove Non-Alphabetic Characters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1f82acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_RE   = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "HTML_RE  = re.compile(r'<.*?>')\n",
    "NONALPH  = re.compile(r'[^a-z\\s]+')     # keep letters & spaces only\n",
    "WS_RE    = re.compile(r'\\s+')\n",
    "\n",
    "# Defining Preprocessing Function\n",
    "def _keep_alpha_only(text: str) -> str:\n",
    "    text = NONALPH.sub(\" \", text)    # remove non-letters\n",
    "    text = WS_RE.sub(\" \", text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da527af",
   "metadata": {},
   "source": [
    "### **3.3 Lemmatization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32faae38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lemmatizer and stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Define preprocessing + lemmatization function\n",
    "def preprocess_and_lemmatize(text):\n",
    "    if isinstance(text, str):  # make sure it's a string\n",
    "        # Lowercase\n",
    "        text = text.lower()\n",
    "\n",
    "        # Remove punctuation, numbers, special chars\n",
    "        text = re.sub(r'[^a-z\\s]', '', text)\n",
    "\n",
    "        # Tokenize\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "\n",
    "        # Remove stopwords + lemmatize\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "\n",
    "        return \" \".join(tokens)\n",
    "    else:\n",
    "        return \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e6a4d2",
   "metadata": {},
   "source": [
    "### **3.4 Apply Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621c251c",
   "metadata": {},
   "source": [
    "**Defining Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec713ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining function to apply preprocessing\n",
    "def apply_preprocessing(text: str) -> str:\n",
    "    \"\"\"\n",
    "      1) preprocess_text_lowercase_url  [lowercase + URL removal + whitespace clean]\n",
    "      2) _keep_alpha_only                [remove non-alphabetic, collapse spaces]\n",
    "      3) preprocess_and_lemmatize [tokenize, drop stopwords, lemmatize]\n",
    "    \"\"\"\n",
    "    # Step 1 (Teammate 4)\n",
    "    text = preprocess_text_lowercase_url(text)\n",
    "\n",
    "    # Step 2 (Teammate 4)\n",
    "    text = _keep_alpha_only(text)\n",
    "\n",
    "    # Step 3 (Teammate 5, adapter)\n",
    "    text = preprocess_and_lemmatize(text)\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c5e64b",
   "metadata": {},
   "source": [
    "**Applying Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9283bdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['title_clean'] = df['title'].apply(apply_preprocessing)\n",
    "df['text_clean'] = df['text'].apply(apply_preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ad932f",
   "metadata": {},
   "source": [
    "### **3.5 Creating `combined_text` column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c726a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"combined_text\"] = (df[\"title_clean\"] + \" \" + df[\"text_clean\"]).str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d31e426b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>label</th>\n",
       "      <th>title_clean</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>combined_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ben Stein Calls Out 9th Circuit Court: Committ...</td>\n",
       "      <td>21st Century Wire says Ben Stein, reputable pr...</td>\n",
       "      <td>US_News</td>\n",
       "      <td>February 13, 2017</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>ben stein call th circuit court committed coup...</td>\n",
       "      <td>st century wire say ben stein reputable profes...</td>\n",
       "      <td>ben stein call th circuit court committed coup...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Trump drops Steve Bannon from National Securit...</td>\n",
       "      <td>WASHINGTON (Reuters) - U.S. President Donald T...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>April 5, 2017</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>trump drop steve bannon national security council</td>\n",
       "      <td>washington reuters u president donald trump re...</td>\n",
       "      <td>trump drop steve bannon national security coun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Puerto Rico expects U.S. to lift Jones Act shi...</td>\n",
       "      <td>(Reuters) - Puerto Rico Governor Ricardo Rosse...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>September 27, 2017</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>puerto rico expects u lift jones act shipping ...</td>\n",
       "      <td>reuters puerto rico governor ricardo rossello ...</td>\n",
       "      <td>puerto rico expects u lift jones act shipping ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OOPS: Trump Just Accidentally Confirmed He Le...</td>\n",
       "      <td>On Monday, Donald Trump once again embarrassed...</td>\n",
       "      <td>News</td>\n",
       "      <td>May 22, 2017</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>oops trump accidentally confirmed leaked israe...</td>\n",
       "      <td>monday donald trump embarrassed country accide...</td>\n",
       "      <td>oops trump accidentally confirmed leaked israe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Donald Trump heads for Scotland to reopen a go...</td>\n",
       "      <td>GLASGOW, Scotland (Reuters) - Most U.S. presid...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>June 24, 2016</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>donald trump head scotland reopen golf resort</td>\n",
       "      <td>glasgow scotland reuters u presidential candid...</td>\n",
       "      <td>donald trump head scotland reopen golf resort ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Ben Stein Calls Out 9th Circuit Court: Committ...   \n",
       "1  Trump drops Steve Bannon from National Securit...   \n",
       "2  Puerto Rico expects U.S. to lift Jones Act shi...   \n",
       "3   OOPS: Trump Just Accidentally Confirmed He Le...   \n",
       "4  Donald Trump heads for Scotland to reopen a go...   \n",
       "\n",
       "                                                text       subject  \\\n",
       "0  21st Century Wire says Ben Stein, reputable pr...       US_News   \n",
       "1  WASHINGTON (Reuters) - U.S. President Donald T...  politicsNews   \n",
       "2  (Reuters) - Puerto Rico Governor Ricardo Rosse...  politicsNews   \n",
       "3  On Monday, Donald Trump once again embarrassed...          News   \n",
       "4  GLASGOW, Scotland (Reuters) - Most U.S. presid...  politicsNews   \n",
       "\n",
       "                  date label  \\\n",
       "0    February 13, 2017  FAKE   \n",
       "1       April 5, 2017   TRUE   \n",
       "2  September 27, 2017   TRUE   \n",
       "3         May 22, 2017  FAKE   \n",
       "4       June 24, 2016   TRUE   \n",
       "\n",
       "                                         title_clean  \\\n",
       "0  ben stein call th circuit court committed coup...   \n",
       "1  trump drop steve bannon national security council   \n",
       "2  puerto rico expects u lift jones act shipping ...   \n",
       "3  oops trump accidentally confirmed leaked israe...   \n",
       "4      donald trump head scotland reopen golf resort   \n",
       "\n",
       "                                          text_clean  \\\n",
       "0  st century wire say ben stein reputable profes...   \n",
       "1  washington reuters u president donald trump re...   \n",
       "2  reuters puerto rico governor ricardo rossello ...   \n",
       "3  monday donald trump embarrassed country accide...   \n",
       "4  glasgow scotland reuters u presidential candid...   \n",
       "\n",
       "                                       combined_text  \n",
       "0  ben stein call th circuit court committed coup...  \n",
       "1  trump drop steve bannon national security coun...  \n",
       "2  puerto rico expects u lift jones act shipping ...  \n",
       "3  oops trump accidentally confirmed leaked israe...  \n",
       "4  donald trump head scotland reopen golf resort ...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55757f93",
   "metadata": {},
   "source": [
    "**Checking length of combined texts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9e0fd93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Char length stats: {'count': 44898.0, 'mean': 1752.6512762261125, 'std': 1504.6273547302997, 'min': 0.0, '50%': 1542.0, '90%': 3202.0, '95%': 3917.300000000003, '99%': 6340.029999999999, 'max': 37972.0}\n",
      "Token length stats: {'count': 44898.0, 'mean': 242.22885206467993, 'std': 204.33721510097453, 'min': 0.0, '50%': 215.0, '90%': 440.0, '95%': 538.0, '99%': 863.0, 'max': 4968.0}\n",
      "{'too_short': 9, 'too_long_char': 266}\n"
     ]
    }
   ],
   "source": [
    "# Character lengths\n",
    "COL = \"combined_text\" \n",
    "df[\"_char_len\"] = df[COL].str.len()\n",
    "\n",
    "# Simple whitespace token count [plain: word count proxy]\n",
    "df[\"_tok_len_ws\"] = df[COL].str.split().apply(len)\n",
    "\n",
    "# Summaries\n",
    "print(\"Char length stats:\", df[\"_char_len\"].describe(percentiles=[.5,.9,.95,.99]).to_dict())\n",
    "print(\"Token length stats:\", df[\"_tok_len_ws\"].describe(percentiles=[.5,.9,.95,.99]).to_dict())\n",
    "\n",
    "# Flags for extremes (adjust thresholds to your data)\n",
    "too_short = df[\"_tok_len_ws\"] < 3           # [plain: likely junk]\n",
    "too_long_char = df[\"_char_len\"] > 8000      # [plain: abnormally long articles]\n",
    "print({\"too_short\": int(too_short.sum()), \"too_long_char\": int(too_long_char.sum())})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fb99a3",
   "metadata": {},
   "source": [
    "**Insights**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adab34d8",
   "metadata": {},
   "source": [
    "Since some combined texts are too long, those training BERT and LSTM should set paramters for the maximum length allowed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7955e020",
   "metadata": {},
   "source": [
    "## **Chapter 4. Modelling & Evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9be104",
   "metadata": {},
   "source": [
    "In this section we will implement 3 different models on our `clean_text` and `combined_text` columns.\n",
    "\n",
    "The models are:\n",
    "- `Logistic Regression`\n",
    "- `LSTM`\n",
    "- `BERT`\n",
    "\n",
    "We will also do `evaluation` for each of the models together with training due to the training for some of the models requiring to be done in external environments with GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947a2559",
   "metadata": {},
   "source": [
    "### **4.1 Stratified Train Test Split**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10726d9",
   "metadata": {},
   "source": [
    "We create a function to do two Stratified Train Test Splits to our data ensruing that we have 10% in both the validation split and the test split.\n",
    "\n",
    "A Stratified Split ensures we maintain the ratio of classes `Main/Fake` throughhout our splits\n",
    "\n",
    "We export the splits into csv files for GPU training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a9aec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 35918, 'val': 4490, 'test': 4490}\n"
     ]
    }
   ],
   "source": [
    "def train_val_test_split_stratified(df, label_col=\"label\", test_size=0.1, val_size=0.1, seed=42):\n",
    "    y = df[label_col].values  # these are \"FAKE\"/\"TRUE\" strings\n",
    "\n",
    "    # First split: train+val vs test\n",
    "    sss1 = StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=seed)\n",
    "    idx_trainval, idx_test = next(sss1.split(df, y))\n",
    "    df_trainval = df.iloc[idx_trainval].reset_index(drop=True)\n",
    "    df_test = df.iloc[idx_test].reset_index(drop=True)\n",
    "\n",
    "    # Second split: train vs val\n",
    "    sss2 = StratifiedShuffleSplit(n_splits=1, test_size=val_size/(1-test_size), random_state=seed)\n",
    "    y_tv = df_trainval[label_col].values\n",
    "    idx_train, idx_val = next(sss2.split(df_trainval, y_tv))\n",
    "    df_train = df_trainval.iloc[idx_train].reset_index(drop=True)\n",
    "    df_val = df_trainval.iloc[idx_val].reset_index(drop=True)\n",
    "\n",
    "    return df_train, df_val, df_test\n",
    "\n",
    "# Run the split\n",
    "df_train, df_val, df_test = train_val_test_split_stratified(\n",
    "    df, label_col=\"label\", test_size=0.10, val_size=0.10, seed=42\n",
    ")\n",
    "\n",
    "# Print sizes\n",
    "print({k: len(v) for k,v in {\"train\": df_train, \"val\": df_val, \"test\": df_test}.items()})\n",
    "\n",
    "# Save the raw splits\n",
    "from pathlib import Path\n",
    "split_dir = Path(\"../data\")        # define as Path, not string\n",
    "split_dir.mkdir(parents=True, exist_ok=True)  # make sure folder exists\n",
    "\n",
    "df_train.to_csv(split_dir / \"train.csv\", index=False)\n",
    "df_val.to_csv(split_dir / \"val.csv\", index=False)\n",
    "df_test.to_csv(split_dir / \"test.csv\", index=False)\n",
    "print(\"Files saved succesfully under data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4794f6e7",
   "metadata": {},
   "source": [
    "We export `train.csv`,  `test.csv` & `val.csv` to kaggle as a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89de5e2d",
   "metadata": {},
   "source": [
    "### **Kaggle Configuration**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8dc9f23",
   "metadata": {},
   "source": [
    "We define paths that suit both the local repository paths and the kaggle directories. The notebook falls on either depending on where the notebook is being run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6788b6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Portable paths: Kaggle vs Local ---\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "IS_KAGGLE = Path(\"/kaggle/working\").exists()\n",
    "\n",
    "WORK_DIR   = Path(\"/kaggle/working\") if IS_KAGGLE else Path(\".\").resolve()\n",
    "DATA_OUT   = (WORK_DIR / \"data\")      if IS_KAGGLE else Path(\"../data\")\n",
    "MODELS_OUT = (WORK_DIR / \"models\")    if IS_KAGGLE else Path(\"models\")\n",
    "RESULTS_OUT= (WORK_DIR / \"results\")   if IS_KAGGLE else Path(\"../results\")\n",
    "DATA_DIR = Path(\"/kaggle/input/fake-news-split\") if IS_KAGGLE else Path(\"../data\")\n",
    "\n",
    "# Make sure they exist if/when used\n",
    "for p in [DATA_OUT, MODELS_OUT, RESULTS_OUT]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def save_artifact(df, filename: str):\n",
    "    \"\"\"\n",
    "    Save a pandas DataFrame both to Kaggle (/kaggle/working/...)\n",
    "    and, if present locally, to ../data. Falls back to CWD.\n",
    "    \"\"\"\n",
    "    written = []\n",
    "    # Kaggle\n",
    "    if IS_KAGGLE:\n",
    "        out_k = WORK_DIR / filename\n",
    "        df.to_csv(out_k, index=False)\n",
    "        print(f\"[Kaggle] Saved: {out_k}\")\n",
    "        written.append(str(out_k))\n",
    "    # Local ../data\n",
    "    local_dir = Path(\"../data\")\n",
    "    if local_dir.exists():\n",
    "        out_l = local_dir / filename\n",
    "        df.to_csv(out_l, index=False)\n",
    "        print(f\"[Local]  Saved: {out_l}\")\n",
    "        written.append(str(out_l))\n",
    "    # Fallback\n",
    "    if not written:\n",
    "        out_f = Path(\"./\") / filename\n",
    "        df.to_csv(out_f, index=False)\n",
    "        print(f\"[Fallback] Saved: {out_f}\")\n",
    "        written.append(str(out_f))\n",
    "    return written\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a033ad5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(DATA_DIR / \"train.csv\")\n",
    "df_val   = pd.read_csv(DATA_DIR / \"val.csv\")\n",
    "df_test  = pd.read_csv(DATA_DIR / \"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380f30cf",
   "metadata": {},
   "source": [
    "### **4.2 Logistic Regression on Text Only**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cf6858",
   "metadata": {},
   "source": [
    "Before moving to advanced models , we first establish a simple baseline using TF-IDF vectorization combined with Logistic Regression. This provides a benchmark to compare more complex approaches against."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64223a5d",
   "metadata": {},
   "source": [
    "#### **Step 1. Vectorize text with TF-IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcefe087",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=10000,     # top 10k features\n",
    "    ngram_range=(1,2),      # unigrams + bigrams\n",
    "    stop_words=\"english\"    # remove stopwords\n",
    ")\n",
    "\n",
    "label_map = {\"FAKE\": 1, \"TRUE\": 0}\n",
    "for d in (df_train, df_val, df_test):\n",
    "    d[\"label_num\"] = d['label'].map(label_map)\n",
    "    \n",
    "X_train = vectorizer.fit_transform(df_train[\"text_clean\"])\n",
    "X_val   = vectorizer.transform(df_val[\"text_clean\"])\n",
    "X_test  = vectorizer.transform(df_test[\"text_clean\"])\n",
    "\n",
    "y_train = df_train[\"label_num\"]\n",
    "y_val   = df_val[\"label_num\"]\n",
    "y_test  = df_test[\"label_num\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497922c2",
   "metadata": {},
   "source": [
    "#### **Step 2. Train logistic regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07b7885",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(\n",
    "    max_iter=500,       # enough iterations\n",
    "    solver=\"liblinear\"  # good for small/medium datasets\n",
    ")\n",
    "log_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b70e52c",
   "metadata": {},
   "source": [
    "#### **Step 3. Evaluate on Validation Set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fdb4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_pred = log_reg.predict(X_val)\n",
    "print(\"Validation Accuracy:\", accuracy_score(y_val, y_val_pred))\n",
    "print(\"\\nClassification Report (Validation):\\n\", classification_report(y_val, y_val_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fe5d41",
   "metadata": {},
   "source": [
    "#### **Step 4. Evaluate on Test Set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb19f58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = log_reg.predict(X_test)\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_test_pred))\n",
    "print(\"\\nClassification Report (Test):\\n\", classification_report(y_test, y_test_pred))\n",
    "y_pred_lr_clean = y_test_pred  # alias for comparison table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea13647",
   "metadata": {},
   "source": [
    "### **4.3 Logistic Regression on Combined Text**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac528cbe",
   "metadata": {},
   "source": [
    "#### **Step 1. Train/Validation/Test Setup**\n",
    "\n",
    "We use stratified splits to ensure balanced representation of FAKE and TRUE labels across train, validation, and test sets.  \n",
    "- Labels are encoded as FAKE = 1, TRUE = 0.  \n",
    "- Text features are taken from the `combined_text` column (or `text` if not available).  \n",
    "- The resulting splits are: Train, Validation, and Test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1671e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and labels using prepared splits: df_train, df_val, df_test\n",
    "\n",
    "TEXT_COL = \"combined_text\" if \"combined_text\" in df_train.columns else \"text\"\n",
    "LABEL_COL = \"label\"\n",
    "\n",
    "# Encode labels: FAKE = 1, TRUE = 0\n",
    "label_map = {\"FAKE\": 1, \"TRUE\": 0}\n",
    "for d in (df_train, df_val, df_test):\n",
    "    d[\"label_num\"] = d[LABEL_COL].map(label_map)\n",
    "\n",
    "# Split into features (X) and targets (y)\n",
    "X_train, y_train = df_train[TEXT_COL], df_train[\"label_num\"]\n",
    "X_val,   y_val   = df_val[TEXT_COL],   df_val[\"label_num\"]\n",
    "X_test,  y_test  = df_test[TEXT_COL],  df_test[\"label_num\"]\n",
    "\n",
    "print(\"Splits:\", df_train.shape, df_val.shape, df_test.shape, \"| text column:\", TEXT_COL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1960d48",
   "metadata": {},
   "source": [
    "#### **Step 2. Baseline Model: TF-IDF + Logistic Regression**\n",
    "\n",
    "We fit a simple baseline using TF-IDF features (unigrams, max 5k terms) and Logistic Regression.  \n",
    "This establishes a reference for accuracy, F1, and ROC-AUC before tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890b1391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize\n",
    "tfidf_base = TfidfVectorizer(max_features=5000, stop_words=\"english\")\n",
    "Xtr_base = tfidf_base.fit_transform(X_train)\n",
    "Xte_base = tfidf_base.transform(X_test)\n",
    "\n",
    "# Model\n",
    "lr_base = LogisticRegression(max_iter=1000, solver=\"liblinear\", random_state=42)\n",
    "lr_base.fit(Xtr_base, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_base  = lr_base.predict(Xte_base)\n",
    "y_proba_base = lr_base.predict_proba(Xte_base)[:, 1]\n",
    "y_pred_lr_combined = y_pred_base  # alias for comparison table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5b3bd6",
   "metadata": {},
   "source": [
    "#### **Step 3. Baseline Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7082954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Baseline Logistic Regression\n",
    "print(\"Baseline Accuracy:\", round(accuracy_score(y_test, y_pred_base), 4))\n",
    "print(\"\\nClassification Report (Baseline):\\n\", classification_report(y_test, y_pred_base, target_names=[\"TRUE\",\"FAKE\"]))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm_base = confusion_matrix(y_test, y_pred_base)\n",
    "ConfusionMatrixDisplay(cm_base, display_labels=[\"TRUE\",\"FAKE\"]).plot(values_format=\"d\")\n",
    "plt.title(\"Baseline — Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# ROC-AUC Curve\n",
    "auc_base = roc_auc_score(y_test, y_proba_base)\n",
    "fpr_b, tpr_b, _ = roc_curve(y_test, y_proba_base)\n",
    "plt.plot(fpr_b, tpr_b, label=f\"Baseline ROC-AUC = {auc_base:.4f}\")\n",
    "plt.plot([0,1],[0,1],\"--\")\n",
    "plt.xlabel(\"False Positive Rate\"); plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Baseline — ROC Curve\"); plt.legend(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc71ee1",
   "metadata": {},
   "source": [
    "**Model Performance Summary**\n",
    "\n",
    "Accuracy: 0.9846 (~99%)\n",
    "\n",
    "Precision: TRUE = 0.98, FAKE = 0.99\n",
    "\n",
    "Recall: TRUE = 0.99, FAKE = 0.98\n",
    "\n",
    "F1-score: Both classes ~0.99\n",
    "\n",
    "ROC-AUC: 0.9988 (excellent separation between classes)\n",
    "\n",
    "\n",
    "**Confusion Matrix Insights**\n",
    "\n",
    "TRUE articles: 2119 correctly predicted, 23 misclassified as FAKE.\n",
    "\n",
    "FAKE articles: 2302 correctly predicted, 46 misclassified as TRUE.\n",
    "\n",
    "The model makes very few mistakes compared to the large sample size.\n",
    "\n",
    "* **Interpretation:**\n",
    "\n",
    "Errors are balanced between both classes - the model is not biased towards TRUE or FAKE.\n",
    "\n",
    "This means misclassifications happens in fewer than ~70 out of ~4490 teest samples (<1.6%) \n",
    "\n",
    "\n",
    "**ROC Curve**\n",
    "\n",
    "ROC-AUC = 0.9988 (~99.9%) - that means the model can almost perfectly distinguish between fake and true news.\n",
    "\n",
    "The curve hugs the top-left corner which is an indication of very high sensitivity and specificity.\n",
    "\n",
    "\n",
    "**Key Takeaways**\n",
    "\n",
    "- The baseline Logistic Regression model performs exceptionally well, with nearly perfect accuracy and ROC-AUC.\n",
    "\n",
    "- Both classes (TRUE, FAKE) are balanced in performance, so the model is not biased toward one class.\n",
    "\n",
    "- The few misclassifications (23 TRUE → FAKE, 46 FAKE → TRUE) are very small relative to the dataset size.\n",
    "\n",
    "This strong baseline suggests Logistic Regression with TF-IDF is already a very competitive model for fake news detection.\n",
    "\n",
    "Hyperparameter tuning may improve results slightly, but even the untuned model is strong enough for deployment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5164e1fb",
   "metadata": {},
   "source": [
    "**Save Baseline Model — For Reuse/Deployment**  \n",
    "\n",
    "After evaluating the baseline Logistic Regression, we save the model and vectorizer as artifacts.  \n",
    "These files can be reloaded later for deployment (e.g., Streamlit app, API, or reporting).  \n",
    "This ensures we preserve a strong baseline, even if tuning results differ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1925e8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, joblib\n",
    "\n",
    "#Create models directory if doesn't exists\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "#Save baseline Logistic Regression + TF-IDF\n",
    "joblib.dump(lr_base, \"models/logreg_model.joblib\")\n",
    "joblib.dump(tfidf_base,   \"models/tfidf_vectorizer.joblib\")\n",
    "print(\"Saved: models/logreg_model.joblib, models/tfidf_vectorizer.joblib\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe731cdd",
   "metadata": {},
   "source": [
    "#### **Step 4. Hyperparameter Tuning with GridSearchCV**\n",
    "\n",
    "Now that we have a strong baseline, we perform hyperparameter tuning to confirm robustness and test whether performance can be further optimized.\n",
    "\n",
    "We tune both TF-IDF and Logistic Regression:\n",
    "\n",
    "- `tfidf__ngram_range`: unigrams vs bigrams  \n",
    "- `tfidf__max_df` and `tfidf__min_df`: filter overly common/rare words  \n",
    "- `clf__C`: regularization strength  \n",
    "- `clf__solver` and `clf__penalty`: logistic regression optimization  \n",
    "\n",
    "Evaluation metric: **F1-macro** (balances FAKE and TRUE equally)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4359e363",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "\n",
    "# Pipeline: TF-IDF + Logistic Regression\n",
    "pipe = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(stop_words=\"english\")),\n",
    "    (\"clf\", LogisticRegression(max_iter=2000, random_state=42))\n",
    "])\n",
    "\n",
    "# Parameter grid\n",
    "param_grid = {\n",
    "    \"tfidf__max_df\": [0.5, 0.7, 0.9],\n",
    "    \"tfidf__min_df\": [2, 5],\n",
    "    \"tfidf__ngram_range\": [(1,1), (1,2)],\n",
    "    \"clf__C\": [0.1, 1, 3, 10],\n",
    "    \"clf__solver\": [\"liblinear\"],\n",
    "    \"clf__penalty\": [\"l2\"]\n",
    "}\n",
    "\n",
    "# Cross-validation strategy\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Run GridSearch\n",
    "gs = GridSearchCV(pipe, param_grid, scoring=\"f1_macro\", cv=cv, n_jobs=-1, verbose=1)\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best CV f1_macro:\", round(gs.best_score_, 4))\n",
    "print(\"Best params:\", gs.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9825fa",
   "metadata": {},
   "source": [
    "#### **Step 5. Tuned Model Evaluation**\n",
    "\n",
    "We evaluate the best configuration on the held-out test set and compare to baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afbcfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate best model on test set\n",
    "best = gs.best_estimator_\n",
    "\n",
    "pred_tuned= best.predict(X_test)\n",
    "proba_tuned = best.predict_proba(X_test)[:,1]\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "print(\"Tuned Accuracy:\", round(accuracy_score(y_test, pred_tuned), 4))\n",
    "print(\"Tuned F1 (macro):\", round(f1_score(y_test, pred_tuned, average=\"macro\"), 4))\n",
    "print(\"\\nClassification Report (Tuned):\\n\",\n",
    "      classification_report(y_test, pred_tuned, target_names=[\"TRUE\",\"FAKE\"]))\n",
    "\n",
    "cm_tuned = confusion_matrix(y_test, pred_tuned)\n",
    "ConfusionMatrixDisplay(cm_tuned, display_labels=[\"TRUE\",\"FAKE\"]).plot(values_format=\"d\")\n",
    "plt.title(\"Tuned — Confusion Matrix\"); plt.show()\n",
    "\n",
    "auc_tuned = roc_auc_score(y_test, proba_tuned)\n",
    "fpr_t, tpr_t, _ = roc_curve(y_test, proba_tuned)\n",
    "plt.plot(fpr_t, tpr_t, label=f\"Tuned ROC-AUC = {auc_tuned:.4f}\")\n",
    "plt.plot([0,1],[0,1],\"--\")\n",
    "plt.xlabel(\"False Positive Rate\"); plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Tuned — ROC Curve\"); plt.legend(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1d6ffd",
   "metadata": {},
   "source": [
    "**F. Baseline vs Tuned Model Comparison**\n",
    "\n",
    "We compare baseline Logistic Regression with the tuned GridSearchCV model to highlight improvements.\n",
    "\n",
    "| Model                      | Accuracy | F1 (Macro) | ROC-AUC |\n",
    "|-----------------------------|----------|------------|---------|\n",
    "| Logistic Regression (Base)  | 0.9846   | ~0.98    | ~0.9988  |\n",
    "| Logistic Regression (Tuned) | 0.9922   | ~0.99  | ~0.9997 |\n",
    "\n",
    "**Interpretation:**\n",
    "- The baseline model was already very strong, with high precision, recall, and ROC-AUC.\n",
    "- After tuning, the **tuned model** achieved slightly higher Accuracy (+0.0076), F1 (+0.0076), and ROC-AUC (+0.0009).  \n",
    "- Although the improvement is modest, tuning confirms that the model is **robust and reliable across multiple folds**, making it safer for deployment.  \n",
    "- Both models perform well, but the tuned version is recommended for production since it has been validated systematically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b641416",
   "metadata": {},
   "source": [
    "**Step 6. Save Tuned Model Artifacts - For Deployment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103e3b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, joblib\n",
    "\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "# Save tuned model and vectorizer\n",
    "joblib.dump(best, \"models/logreg_tuned_model.joblib\")\n",
    "joblib.dump(best.named_steps[\"tfidf\"], \"models/tfidf_tuned_vectorizer.joblib\")\n",
    "\n",
    "print(\"Saved: models/logreg_tuned_model.joblib\")\n",
    "print(\"Saved: models/tfidf_tuned_vectorizer.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c1dae6",
   "metadata": {},
   "source": [
    "### **4.4 LSTM on Text Only**\n",
    "\n",
    "This step was exported to kaggle to utilise its GPU for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd1406f",
   "metadata": {},
   "source": [
    "#### **Step 1. Config**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3717a3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# Input/output\n",
    "INPUT_COL = \"text_clean\"   # or \"text_clean\"\n",
    "LABEL_COL = \"label\"\n",
    "\n",
    "# Tokenizer / sequence\n",
    "NUM_WORDS = 50000\n",
    "MAX_LEN   = 256\n",
    "OOV_TOKEN = \"<UNK>\"\n",
    "\n",
    "# Model settings\n",
    "EMBED_DIM  = 128\n",
    "LSTM_UNITS = 128\n",
    "DROPOUT    = 0.3\n",
    "LR         = 1e-3\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS     = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a019c9bc",
   "metadata": {},
   "source": [
    "#### **Step 2. Normalize Labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87551557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize labels\n",
    "for df in [df_train, df_val, df_test]:\n",
    "    df[LABEL_COL] = df[LABEL_COL].astype(str).str.upper().str.strip()\n",
    "    df[INPUT_COL] = df[INPUT_COL].astype(str).str.strip()\n",
    "\n",
    "# Convert labels to 0/1\n",
    "def labels_to_int(series):\n",
    "    return (series == \"TRUE\").astype(int).values\n",
    "\n",
    "y_tr = labels_to_int(df_train[LABEL_COL])\n",
    "y_va = labels_to_int(df_val[LABEL_COL])\n",
    "y_te = labels_to_int(df_test[LABEL_COL])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0671fa",
   "metadata": {},
   "source": [
    "#### **Step 3. Tokenize and Pad**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3aec49",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = Tokenizer(num_words=NUM_WORDS, lower=True, oov_token=OOV_TOKEN)\n",
    "tok.fit_on_texts(df_train[INPUT_COL])\n",
    "\n",
    "def to_padded(texts, tokenizer, max_len=MAX_LEN):\n",
    "    seqs = tokenizer.texts_to_sequences(texts)\n",
    "    return pad_sequences(seqs, maxlen=max_len, padding=\"post\", truncating=\"post\")\n",
    "\n",
    "X_tr = to_padded(df_train[INPUT_COL], tok)\n",
    "X_va = to_padded(df_val[INPUT_COL], tok)\n",
    "X_te = to_padded(df_test[INPUT_COL], tok)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb41e944",
   "metadata": {},
   "source": [
    "#### **Step 4. Build Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbd6149",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, max_len):\n",
    "    inp = layers.Input(shape=(max_len,), dtype=\"int32\")\n",
    "    emb = layers.Embedding(input_dim=vocab_size,\n",
    "                           output_dim=EMBED_DIM,\n",
    "                           mask_zero=True)(inp)\n",
    "    x = layers.Bidirectional(layers.LSTM(LSTM_UNITS, return_sequences=True))(emb)\n",
    "    x = layers.GlobalMaxPooling1D()(x)\n",
    "    x = layers.Dropout(DROPOUT)(x)\n",
    "    x = layers.Dense(128, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(DROPOUT)(x)\n",
    "    out = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = models.Model(inp, out)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=LR),\n",
    "                  loss=\"binary_crossentropy\",\n",
    "                  metrics=[tf.keras.metrics.AUC(name=\"auc\"), \"accuracy\"])\n",
    "    return model\n",
    "\n",
    "vocab_size = min(NUM_WORDS, len(tok.word_index) + 1)\n",
    "model = build_model(vocab_size, MAX_LEN)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03da6ef7",
   "metadata": {},
   "source": [
    "#### **Step 5. Train the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f57a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(monitor=\"val_auc\", mode=\"max\",\n",
    "                                     patience=2, restore_best_weights=True)\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    X_tr, y_tr,\n",
    "    validation_data=(X_va, y_va),\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb06cc55",
   "metadata": {},
   "source": [
    "#### **Step 6. Saving the model and Tokenizers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b503b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "model.save(WORK_DIR / \"best_lstm.keras\")\n",
    "\n",
    "# Save the tokenizer\n",
    "import json\n",
    "with open(WORK_DIR / \"tokenizer.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(tok.to_json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18f086e",
   "metadata": {},
   "source": [
    "#### **Step 7. Metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c624ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob = model.predict(X_te, batch_size=2*BATCH_SIZE).ravel()\n",
    "y_pred = (y_prob >= 0.5).astype(int)\n",
    "\n",
    "acc = accuracy_score(y_te, y_pred)\n",
    "prec, rec, f1, _ = precision_recall_fscore_support(y_te, y_pred, average=\"binary\")\n",
    "roc = roc_auc_score(y_te, y_prob)\n",
    "\n",
    "metrics = {\"accuracy\": float(acc), \"precision\": float(prec),\n",
    "           \"recall\": float(rec), \"f1\": float(f1), \"roc_auc\": float(roc)}\n",
    "metrics\n",
    "y_pred_lstm_clean = y_pred  # alias for comparison table (ground truth is y_te here)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446e54e5",
   "metadata": {},
   "source": [
    "#### **Step 8. Saving Metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643ae120",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(WORK_DIR / \"metrics.json\", \"w\") as f:\n",
    "    json.dump(metrics, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9607c2a1",
   "metadata": {},
   "source": [
    "### **4.5 LSTM on Combined Text**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8940793",
   "metadata": {},
   "source": [
    "**Step 1. Data Preparation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65a6a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    " # ==================== DATA PREPARATION ====================\n",
    "# Use the combined_text column\n",
    "X_train = df_train['combined_text'].fillna('').astype(str).values\n",
    "X_val = df_val['combined_text'].fillna('').astype(str).values\n",
    "X_test = df_test['combined_text'].fillna('').astype(str).values\n",
    "\n",
    "# Convert labels to binary (FAKE=1, TRUE=0)\n",
    "y_train = (df_train['label'] == 'FAKE').astype(int).values\n",
    "y_val = (df_val['label'] == 'FAKE').astype(int).values\n",
    "y_test = (df_test['label'] == 'FAKE').astype(int).values\n",
    "\n",
    "print(f\"Labels - FAKE: {y_train.sum()}, REAL: {len(y_train) - y_train.sum()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ea8d01",
   "metadata": {},
   "source": [
    "#### **Step 2. Tokenization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beab904f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== TOKENIZATION  ====================\n",
    "NUM_WORDS = 50000    # Tokenizer vocabulary size\n",
    "MAX_LEN = 256        # Maximum sequence length  \n",
    "OOV_TOKEN = \"<UNK>\"  # Out-of-vocabulary token\n",
    "    \n",
    "EMBED_DIM = 128      # Embedding dimension\n",
    "LSTM_UNITS = 128     # LSTM units\n",
    "DROPOUT = 0.3        # Dropout rate\n",
    "LR = 1e-3            # Learning rate\n",
    "BATCH_SIZE = 64      # Batch size\n",
    "EPOCHS = 8           # Number of epochs\n",
    "\n",
    "# Create tokenizer \n",
    "tokenizer = Tokenizer(num_words=NUM_WORDS, oov_token=OOV_TOKEN)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# Convert to sequences\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_val_seq = tokenizer.texts_to_sequences(X_val)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad sequences to MAX_LEN\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "X_val_pad = pad_sequences(X_val_seq, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "\n",
    "print(f\"\\nData shapes :\")\n",
    "print(f\"X_train_pad: {X_train_pad.shape}\")\n",
    "print(f\"Vocabulary size: {len(tokenizer.word_index)}\")\n",
    "print(f\"Max sequence length: {MAX_LEN}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0d2ecf",
   "metadata": {},
   "source": [
    "#### **Step 3. Build LSTM Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7ae8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== BUILD LSTM MODEL  ====================\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "def create_lstm_model(vocab_size, embed_dim, lstm_units, dropout_rate, sequence_length):\n",
    "        \"\"\"\n",
    "        Build LSTM model \n",
    "        \"\"\"\n",
    "        model = Sequential([\n",
    "            # Embedding layer with EMBED_DIM\n",
    "            Embedding(\n",
    "                input_dim=vocab_size, \n",
    "                output_dim=embed_dim, \n",
    "                input_length=sequence_length,\n",
    "                name='embedding_layer'\n",
    "            ),\n",
    "            \n",
    "            # Spatial dropout\n",
    "            SpatialDropout1D(dropout_rate, name='spatial_dropout'),\n",
    "            \n",
    "            # Bidirectional LSTM with LSTM_UNITS\n",
    "            Bidirectional(\n",
    "                LSTM(lstm_units, return_sequences=True, dropout=dropout_rate),\n",
    "                name='bidirectional_lstm_1'\n",
    "            ),\n",
    "            \n",
    "            # Second Bidirectional LSTM\n",
    "            Bidirectional(\n",
    "                LSTM(lstm_units // 2, dropout=dropout_rate),  # Half the units for second layer\n",
    "                name='bidirectional_lstm_2'\n",
    "            ),\n",
    "            \n",
    "            # Dense layer\n",
    "            Dense(lstm_units // 2, activation='relu', name='dense_1'),\n",
    "            Dropout(dropout_rate, name='dropout_1'),\n",
    "            \n",
    "            # Output layer\n",
    "            Dense(1, activation='sigmoid', name='output_layer')\n",
    "        ])\n",
    "        \n",
    "        # Compile \n",
    "        optimizer = Adam(learning_rate=LR)\n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss='binary_crossentropy', \n",
    "            metrics=['accuracy', Precision(), Recall()]\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    # Create model \n",
    "vocab_size = min(NUM_WORDS, len(tokenizer.word_index) + 1)\n",
    "lstm_model = create_lstm_model(\n",
    "        vocab_size=vocab_size,\n",
    "        embed_dim=EMBED_DIM,\n",
    "        lstm_units=LSTM_UNITS,\n",
    "        dropout_rate=DROPOUT,\n",
    "        sequence_length=MAX_LEN\n",
    "    )\n",
    "    \n",
    "print(\" Model created successfully!\")\n",
    "print(f\" Model Parameters:\")\n",
    "print(f\"   - Vocabulary size: {vocab_size}\")\n",
    "print(f\"   - Embedding dimension: {EMBED_DIM}\")\n",
    "print(f\"   - LSTM units: {LSTM_UNITS}\")\n",
    "print(f\"   - Dropout rate: {DROPOUT}\")\n",
    "print(f\"   - Learning rate: {LR}\")\n",
    "print(f\"   - Batch size: {BATCH_SIZE}\")\n",
    "print(f\"   - Epochs: {EPOCHS}\")\n",
    "    \n",
    "lstm_model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a279fd96",
   "metadata": {},
   "source": [
    "#### **Step 4. Train Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b355f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== TRAIN MODEL ====================\n",
    "early_stopping = EarlyStopping(\n",
    "        monitor='val_loss', \n",
    "        patience=3,  # Smaller patience for 8 epochs\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "print(f\"\\n Starting training...\")\n",
    "print(f\"   - Batch size: {BATCH_SIZE}\")\n",
    "print(f\"   - Epochs: {EPOCHS}\")\n",
    "    \n",
    "history = lstm_model.fit(\n",
    "        X_train_pad, y_train,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=EPOCHS,\n",
    "        validation_data=(X_val_pad, y_val),\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=1\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eabea7f",
   "metadata": {},
   "source": [
    "#### **Step 5. Evaluate Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2c01f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== EVALUATE MODEL ====================\n",
    "test_loss, test_accuracy, test_precision, test_recall = lstm_model.evaluate(\n",
    "    X_test_pad, y_test, verbose=0\n",
    ")\n",
    "test_f1 = 2 * (test_precision * test_recall) / (test_precision + test_recall)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\" LSTM MODEL RESULTS \")\n",
    "print(\"=\"*60)\n",
    "print(f\" Accuracy:  {test_accuracy:.4f}\")\n",
    "print(f\" Precision: {test_precision:.4f}\")\n",
    "print(f\" Recall:    {test_recall:.4f}\")\n",
    "print(f\"  F1-Score:  {test_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f047e63",
   "metadata": {},
   "source": [
    "#### **Step 6. Make Predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafd7cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== MAKE PREDICTIONS ====================\n",
    "y_pred_proba = lstm_model.predict(X_test_pad, verbose=0)\n",
    "y_pred = (y_pred_proba > 0.5).astype(int).flatten()\n",
    "    \n",
    "# Classification report\n",
    "print(\"\\n Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['REAL', 'FAKE']))\n",
    "    \n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\" Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "y_pred_lstm_combined = y_pred  # alias for comparison table (ground truth is y_test here)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9191ea33",
   "metadata": {},
   "source": [
    "#### **Step 7. Visualise Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b546b97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== VISUALIZE RESULTS ====================\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Accuracy plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy', linewidth=2)\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
    "plt.title('Model Accuracy ', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Loss plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "plt.title('Model Loss ', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/lstm_training_leader_params.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Confusion matrix heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['REAL', 'FAKE'], \n",
    "            yticklabels=['REAL', 'FAKE'],\n",
    "            annot_kws={\"size\": 14})\n",
    "plt.title('LSTM Confusion Matrix ', fontsize=16, fontweight='bold')\n",
    "plt.ylabel('Actual Label', fontsize=12)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.savefig('../results/lstm_confusion_leader_params.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706a1997",
   "metadata": {},
   "source": [
    "#### **Step 8 . Save Model and Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9aacf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== SAVE MODEL AND RESULTS ====================\n",
    "# Save the model\n",
    "model_path = '../models/lstm_fake_news_model.h5'\n",
    "lstm_model.save(model_path)\n",
    "print(f\"\\n Model saved as '{model_path}'\")\n",
    "    \n",
    "# Save tokenizer\n",
    "import pickle\n",
    "with open('../models/tokenizer.pkl', 'wb') as handle:\n",
    "        pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "print(\" Tokenizer saved as '../models/tokenizer.pkl'\")\n",
    "    \n",
    "# Save results \n",
    "results = {\n",
    "        'parameters_used': {\n",
    "            'NUM_WORDS': NUM_WORDS,\n",
    "            'MAX_LEN': MAX_LEN,\n",
    "            'EMBED_DIM': EMBED_DIM,\n",
    "            'LSTM_UNITS': LSTM_UNITS,\n",
    "            'DROPOUT': DROPOUT,\n",
    "            'LR': LR,\n",
    "            'BATCH_SIZE': BATCH_SIZE,\n",
    "            'EPOCHS': EPOCHS\n",
    "        },\n",
    "        'performance': {\n",
    "            'accuracy': float(test_accuracy),\n",
    "            'precision': float(test_precision),\n",
    "            'recall': float(test_recall),\n",
    "            'f1_score': float(test_f1),\n",
    "            'test_loss': float(test_loss)\n",
    "        },\n",
    "        'confusion_matrix': cm.tolist()\n",
    "    }\n",
    "    \n",
    "import json\n",
    "with open('../results/lstm_results.json', 'w') as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "print(\" Results saved as '../results/lstm_results.json'\")\n",
    "    \n",
    "print(f\"\\n LSTM training completed successfully!\")\n",
    "print(f\" Final Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0823e7cf",
   "metadata": {},
   "source": [
    "### **4.6 BERT Model on Text Only**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca0dd5f",
   "metadata": {},
   "source": [
    "#### **Step 1. Defining Parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80dfcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PARAMS = {\n",
    "    \"model_name\": \"bert-base-uncased\",\n",
    "    \"max_len\": 128,      # shorter length to save on memory\n",
    "    \"batch_size\": 8,\n",
    "    \"learning_rate\": 2e-5,\n",
    "    \"epochs\": 3,\n",
    "    \"train_test_split\": 0.2\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e38eda",
   "metadata": {},
   "source": [
    "#### **Step 2. Encoding Labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723f0e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoding labels\n",
    "#Using hugging face expects numeric value hence changing False/True to O/1\n",
    "\n",
    "lbl_enc = LabelEncoder()\n",
    "df_train[\"label_id\"] = lbl_enc.fit_transform(df_train[\"label\"])\n",
    "df_val[\"label_id\"]   = lbl_enc.transform(df_val[\"label\"])\n",
    "df_test[\"label_id\"]  = lbl_enc.transform(df_test[\"label\"])\n",
    "\n",
    "id2label = {i: l for i,l in enumerate(lbl_enc.classes_)}\n",
    "label2id = {l: i for i,l in enumerate(lbl_enc.classes_)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13e02e0",
   "metadata": {},
   "source": [
    "#### **Step 3. Building Hugging Face Datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2334215",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building hugging face datasets\n",
    "\n",
    "ds_train = Dataset.from_pandas(df_train[[\"text_clean\",\"label_id\"]])\n",
    "ds_val   = Dataset.from_pandas(df_val[[\"text_clean\",\"label_id\"]])\n",
    "ds_test  = Dataset.from_pandas(df_test[[\"text_clean\",\"label_id\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a9ae17",
   "metadata": {},
   "source": [
    "#### **Step 4. Tokenization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a199ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(PARAMS[\"model_name\"], use_fast=True)\n",
    "\n",
    "def tokenize_fn(batch):\n",
    "    texts = [str(t) if t is not None else \"\" for t in batch[\"text_clean\"]]\n",
    "    return tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        padding=False,\n",
    "        max_length=PARAMS[\"max_len\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d48bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping\n",
    "\n",
    "ds_train = ds_train.map(tokenize_fn, batched=True)\n",
    "ds_val   = ds_val.map(tokenize_fn, batched=True)\n",
    "ds_test  = ds_test.map(tokenize_fn, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3a2546",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds_train.column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc9c44b",
   "metadata": {},
   "source": [
    "#### **Step 5. Pytorch Format**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210ca8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 1) Rename label column\n",
    "ds_train = ds_train.rename_column(\"label_id\", \"labels\")\n",
    "ds_val   = ds_val.rename_column(\"label_id\", \"labels\")\n",
    "ds_test  = ds_test.rename_column(\"label_id\", \"labels\")\n",
    "\n",
    "# 2) Remove non-tensor columns (like the original text string)\n",
    "def keep_cols(ds):\n",
    "    keep = [\"input_ids\", \"attention_mask\", \"labels\"]\n",
    "    if \"token_type_ids\" in ds.column_names:\n",
    "        keep.append(\"token_type_ids\")\n",
    "    drop = [c for c in ds.column_names if c not in keep]\n",
    "    return ds.remove_columns(drop)\n",
    "\n",
    "ds_train = keep_cols(ds_train)\n",
    "ds_val   = keep_cols(ds_val)\n",
    "ds_test  = keep_cols(ds_test)\n",
    "\n",
    "# 3) Set torch format with explicit columns\n",
    "cols = ds_train.column_names  # after pruning\n",
    "ds_train.set_format(type=\"torch\", columns=cols)\n",
    "ds_val.set_format(type=\"torch\", columns=cols)\n",
    "ds_test.set_format(type=\"torch\", columns=cols)\n",
    "\n",
    "# 4) Sanity checks\n",
    "print(\"Train columns:\", ds_train.column_names)\n",
    "print(\"GPU available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA device:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d46406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch format\n",
    "ds_train.set_format(\"torch\")\n",
    "ds_val.set_format(\"torch\")\n",
    "ds_test.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6ebab1",
   "metadata": {},
   "source": [
    "#### **Step 6. Defining Data Collator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8245a89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collator\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309e2354",
   "metadata": {},
   "source": [
    "#### **Step 7. Loading the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d991c91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the model\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    PARAMS[\"model_name\"],\n",
    "    num_labels=len(lbl_enc.classes_),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70981638",
   "metadata": {},
   "source": [
    "#### **Step 8. Defining Evaluation Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2326a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "\n",
    "    # Some HF models return (logits,) as a tuple\n",
    "    if isinstance(logits, (tuple, list)):\n",
    "        logits = logits[0]\n",
    "\n",
    "    # Convert to numpy if needed\n",
    "    if hasattr(logits, \"detach\"):  # torch.Tensor\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "    if hasattr(labels, \"detach\"):  # torch.Tensor\n",
    "        labels = labels.detach().cpu().numpy()\n",
    "\n",
    "    # Argmax to predicted class ids\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Ensure 1-D arrays\n",
    "    preds = np.asarray(preds).ravel()\n",
    "    labels = np.asarray(labels).ravel()\n",
    "\n",
    "    return {\n",
    "        \"accuracy\":  accuracy_score(labels, preds),\n",
    "        \"f1\":        f1_score(labels, preds, average=\"weighted\", zero_division=0),\n",
    "        \"precision\": precision_score(labels, preds, average=\"weighted\", zero_division=0),\n",
    "        \"recall\":    recall_score(labels, preds, average=\"weighted\", zero_division=0),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e07581",
   "metadata": {},
   "source": [
    "#### **Step 9. Training Arguments**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8562d910",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bert-fake-news\",\n",
    "    eval_strategy=\"epoch\",      # eval each epoch\n",
    "    save_strategy=\"epoch\",            # save each epoch\n",
    "    learning_rate=PARAMS[\"learning_rate\"],\n",
    "    per_device_train_batch_size=PARAMS[\"batch_size\"],  # 8\n",
    "    per_device_eval_batch_size=PARAMS[\"batch_size\"],\n",
    "    num_train_epochs=PARAMS[\"epochs\"],                  # 3\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=50,\n",
    "    disable_tqdm=False,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    report_to=\"none\",                  # disable W&B logging\n",
    "    fp16=torch.cuda.is_available(),    # faster on Kaggle GPU\n",
    "    dataloader_pin_memory=True,\n",
    "    dataloader_num_workers=0,          # avoid worker hangs on Kaggle\n",
    "    seed=42,\n",
    "    run_name=\"bert_fake_news_base\",    # optional: nice run name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21dd5bd",
   "metadata": {},
   "source": [
    "#### **Step 10. Training the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6995b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=ds_train,\n",
    "    eval_dataset=ds_val,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ce3361",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db3be4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = trainer.evaluate(ds_test)\n",
    "print(results)\n",
    "\n",
    "pred_clean = trainer.predict(ds_test)\n",
    "\n",
    "y_pred_bert_clean = np.argmax(pred_clean.predictions, axis=1)\n",
    "y_true_bert_clean = pred_clean.label_ids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871a0885",
   "metadata": {},
   "source": [
    "### **4.7 BERT Model on Combined Text Only**\n",
    "\n",
    "In this section, we are applying **BERT (Bidirectional Encoder Representations from Transformers)** to predict whether a given news article is *fake* or *true*.  \n",
    "\n",
    "#### Why BERT?\n",
    "- BERT is a **transformer-based model** that has been pre-trained on massive amounts of text.  \n",
    "- It understands **context in both directions** (left and right of a word), which makes it powerful for language understanding tasks.  \n",
    "- For text classification tasks like **fake news detection**, BERT has shown state-of-the-art performance compared to traditional machine learning methods.  \n",
    "\n",
    "#### Workflow\n",
    "We will follow these steps in our modeling pipeline:\n",
    "\n",
    "1. **Install dependencies** – Set up HuggingFace Transformers, PyTorch, and Scikit-learn.  \n",
    "2. **Load data** – Import the prepared train, validation, and test CSVs.  \n",
    "3. **Explore data** – Check dataset shape, column names, and class balance.  \n",
    "4. **Preprocess & tokenize** – Convert raw text into tokens using the bert-base-uncased tokenizer.  \n",
    "5. **Create PyTorch dataset loaders** – Wrap the tokenized inputs and labels into PyTorch Dataset and DataLoader objects for training.  \n",
    "6. **Model setup** – Load BertForSequenceClassification with two labels (fake vs true).  \n",
    "7. **Training loop** – Train BERT with AdamW optimizer, scheduler, and backpropagation.  \n",
    "8. **Evaluation** – Measure accuracy, precision, recall, and F1 score on validation and test data.  \n",
    "9. **Save the model** – Store the trained model and tokenizer for future use.\n",
    "\n",
    "By the end of this workflow, we will have a fine-tuned BERT model that can classify unseen news articles as either fake or true.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbee1874",
   "metadata": {},
   "source": [
    "#### **Step 1. Explore the Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650694e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check unique labels\n",
    "print(\"Unique labels:\", df_train['label'].unique())\n",
    "\n",
    "# Convert labels to numeric (FAKE=0, TRUE=1)\n",
    "label_mapping = {\"FAKE\": 0, \"TRUE\": 1}\n",
    "df_train['label_num'] = df_train['label'].map(label_mapping)\n",
    "df_val['label_num']   = df_val['label'].map(label_mapping)\n",
    "df_test['label_num']  = df_test['label'].map(label_mapping)\n",
    "\n",
    "# Check distribution of classes\n",
    "print(\"Training label distribution:\\n\", df_train['label_num'].value_counts(normalize=True))\n",
    "print(\"Validation label distribution:\\n\", df_val['label_num'].value_counts(normalize=True))\n",
    "print(\"Test label distribution:\\n\", df_test['label_num'].value_counts(normalize=True))\n",
    "\n",
    "# sneak peek after conversion\n",
    "df_train[['combined_text', 'label', 'label_num']].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79c14a7",
   "metadata": {},
   "source": [
    "#### **Step 2. Define Parameters.**\n",
    "\n",
    "At this stage, after inspecting the dataset, we define all key parameters in one dictionary (PARAMS):\n",
    "\n",
    "  **model_name**: the pre-trained model to load (bert-base-uncased is common for English).  \n",
    "  **max_len**: maximum token length for each sequence. This controls how long inputs are padded/truncated during tokenization.  \n",
    "  **batch_size**: number of samples per batch for training.  \n",
    "  **learning_rate**: optimizer learning rate (2e-5 is a typical starting point for BERT).  \n",
    "  **epochs**: how many times we train over the full dataset.  \n",
    "  **device**: whether to run on GPU (cuda) or CPU.\n",
    "\n",
    "Defining these here ensures that tokenization, dataloaders, and model training are consistent and reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a1015f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining all important parameters in one place\n",
    "PARAMS = {\n",
    "    \"model_name\": \"bert-base-uncased\",   # pre-trained BERT model\n",
    "    \"max_len\": 256,                      # max token length for each input\n",
    "    \"batch_size\": 16,                    # batch size for DataLoader\n",
    "    \"learning_rate\": 2e-5,               # learning rate for AdamW optimizer\n",
    "    \"epochs\": 3,                         # number of training epochs\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"  # use GPU if available\n",
    "}\n",
    "\n",
    "\n",
    "PARAMS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbfeb9f",
   "metadata": {},
   "source": [
    "### **Step 3. Tokenization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbec173b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer for BERT\n",
    "tokenizer = BertTokenizer.from_pretrained(PARAMS[\"model_name\"])\n",
    "\n",
    "# sanity check;tokenize a single sentence\n",
    "sample_text = df_train[\"combined_text\"].iloc[0]\n",
    "tokens = tokenizer.encode_plus(\n",
    "    sample_text,\n",
    "    max_length=PARAMS[\"max_len\"],\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\"   # return PyTorch tensors\n",
    ")\n",
    "\n",
    "print(\"Original text:\\n\", sample_text[:200], \"...\\n\")  # first 200 chars\n",
    "print(\"Token IDs:\\n\", tokens[\"input_ids\"])\n",
    "print(\"Attention mask:\\n\", tokens[\"attention_mask\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5c6754",
   "metadata": {},
   "source": [
    "##### When we tokenized a sample, we got two main results:\n",
    "\n",
    " **Input IDs**: Each word or subword is mapped to a numeric ID from BERT’s vocabulary.  \n",
    "    The sequence starts with [CLS] (ID 101) and ends with [SEP] (ID 102).  \n",
    "    If the text is shorter than 256 tokens, the rest is padded with 0s.  \n",
    "    If longer, it is truncated to 256 tokens.  \n",
    "\n",
    " **Attention Mask**: A sequence of 1s and 0s.  \n",
    "    1 means a real token.  \n",
    "    0 means a padding token.  \n",
    "\n",
    "This ensures that all inputs have the same length (max_len=256), and BERT can ignore padding during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d0d54a",
   "metadata": {},
   "source": [
    "#### **Step 4. Create Pytorch dataset loaders**\n",
    "\n",
    "\n",
    "\n",
    "To prepare the data for BERT, we wrap it in a **custom PyTorch Dataset**:\n",
    "\n",
    " **NewsDataset**:  \n",
    "   Takes the cleaned text (combined_text) and labels.  \n",
    "   Uses the BERT tokenizer with our chosen `max_len=256`.  \n",
    "   Returns `input_ids`, `attention_mask`, and `labels` tensors for each sample.  \n",
    "\n",
    "We then create **DataLoaders** for train, validation, and test sets:  \n",
    " These efficiently batch and shuffle the data.  \n",
    " Each batch contains:\n",
    "   input_ids → tokenized text  \n",
    "   attention_mask → marks real tokens vs. padding  \n",
    "   labels → 0 (FAKE) or 1 (TRUE)  \n",
    "\n",
    "These DataLoaders will feed data into BERT during training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0f940d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure labels are clean 1-D ints\n",
    "def ensure_label_vector(series):\n",
    "    arr = np.asarray(series)\n",
    "    if arr.dtype == object:\n",
    "        arr = np.array([int(x[0]) if isinstance(x, (list, np.ndarray)) else int(x) for x in arr], dtype=np.int64)\n",
    "    else:\n",
    "        arr = arr.astype(np.int64)\n",
    "    return arr\n",
    "\n",
    "df_train[\"label_num\"] = ensure_label_vector(df_train[\"label_num\"])\n",
    "df_val[\"label_num\"]   = ensure_label_vector(df_val[\"label_num\"])\n",
    "df_test[\"label_num\"]  = ensure_label_vector(df_test[\"label_num\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0b36cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset as TorchDataset, DataLoader\n",
    "\n",
    "# 1. Create a custom dataset class\n",
    "class NewsDataset(TorchDataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = list(texts)\n",
    "        self.labels = np.asarray(labels, dtype=np.int64)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if isinstance(idx, torch.Tensor):\n",
    "            idx = idx.item()\n",
    "        idx = int(idx)\n",
    "\n",
    "        text = str(self.texts[idx])\n",
    "        label = int(self.labels[idx])\n",
    "\n",
    "        enc = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": enc[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": enc[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long),\n",
    "        }\n",
    "\n",
    "\n",
    "# 2. Helper function to create DataLoaders\n",
    "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
    "    ds = NewsDataset(\n",
    "        texts=df[\"combined_text\"].to_numpy(),\n",
    "        labels=df[\"label_num\"].to_numpy(),\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=max_len\n",
    "    )\n",
    "\n",
    "    return DataLoader(ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# 3. Create DataLoaders for train, val, test\n",
    "train_data_loader = create_data_loader(df_train, tokenizer, PARAMS[\"max_len\"], PARAMS[\"batch_size\"])\n",
    "val_data_loader = create_data_loader(df_val, tokenizer, PARAMS[\"max_len\"], PARAMS[\"batch_size\"])\n",
    "test_data_loader = create_data_loader(df_test, tokenizer, PARAMS[\"max_len\"], PARAMS[\"batch_size\"])\n",
    "\n",
    "len(train_data_loader), len(val_data_loader), len(test_data_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d8b428",
   "metadata": {},
   "source": [
    "#### **Step 5. Model Setup**\n",
    "\n",
    "In this step, we build a custom **BERT-based classifier** for our fake news detection task.  \n",
    "The model architecture is as follows:\n",
    "\n",
    " **BERT Base Model**: We load `bert-base-uncased` as the backbone to generate contextual embeddings from text.  \n",
    " **Dropout Layer**: Added to reduce overfitting by randomly deactivating some neurons during training.  \n",
    " **Linear Layer**: A fully connected layer that maps BERT’s hidden size (768 dimensions) to our two output classes (`FAKE` and `TRUE`).  \n",
    "  **Freezing Option**: We can choose to freeze BERT’s pre-trained layers (`freeze_bert=True`) so only the classifier head trains (faster but may underfit). If set to `False`, the entire model is fine-tuned (slower but usually yields better results).  \n",
    " **Device Placement**: The model is moved to GPU (`cuda`) if available, otherwise runs on CPU.\n",
    "\n",
    "This classifier combines the rich semantic understanding from BERT with a lightweight classification head tailored to our dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef10e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom BERT-based classifier\n",
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, freeze_bert=True):\n",
    "        super(BertClassifier, self).__init__()\n",
    "        \n",
    "        # Load pre-trained BERT model\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        \n",
    "        # Add a dropout layer for regularization\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "        \n",
    "        # Add a linear classifier on top (binary classification: FAKE vs TRUE)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, 2)\n",
    "        \n",
    "        # Optionally freeze BERT weights (so only classifier trains)\n",
    "        if freeze_bert:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Get BERT outputs\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Pooled output is [CLS] token representation\n",
    "        pooled_output = outputs.pooler_output\n",
    "        \n",
    "        # Apply dropout then classifier\n",
    "        x = self.dropout(pooled_output)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate model\n",
    "model = BertClassifier(freeze_bert=False)  # set to True if you want to freeze BERT\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9fe448",
   "metadata": {},
   "source": [
    "**Understanding the BERT Model Architecture**\n",
    "\n",
    "When we print the model, we see the layers and components that make up our `BertClassifier`:\n",
    "\n",
    "1. **`bert` (the backbone)**  \n",
    "    This is the pre-trained **BERT encoder** that processes the text.  \n",
    "    It has:\n",
    "      **Embeddings**:\n",
    "        *Word embeddings*: map each token ID to a 768-dimensional vector.  \n",
    "        *Position embeddings*: capture word order in a sentence (since transformers have no sense of sequence by default).  \n",
    "        *Token type embeddings*: allow BERT to distinguish between sentence A vs sentence B (useful for tasks like QA).  \n",
    "      **Encoder layers**: 12 stacked layers (for `bert-base-uncased`).  \n",
    "        Each layer has **self-attention**, **feed-forward**, and **layer normalization**.  \n",
    "        These layers let BERT capture contextual meaning — e.g., “bank” in *river bank* vs *money bank*.  \n",
    "      **Pooler**: takes the representation of the special `[CLS]` token and transforms it into a fixed-size vector (used for classification).\n",
    "\n",
    "2. **Dropout (0.3)**  \n",
    "    A regularization layer we added to reduce overfitting.  \n",
    "    Randomly “drops” 30% of the neurons during training to make the model more robust.\n",
    "\n",
    "3. **Classifier (Linear layer)**  \n",
    "    Input: 768-dim vector from BERT (the pooled `[CLS]` token).  \n",
    "    Output: 2 logits → `[FAKE, TRUE]`.  \n",
    "    This is the final prediction layer.\n",
    "\n",
    "\n",
    " In short:  \n",
    " The **BERT encoder** extracts deep contextual features from the text.  \n",
    " The **dropout** improves generalization.  \n",
    " The **classifier** maps the features to our labels (fake vs true news).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ac82fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map existing DataLoaders to the expected names\n",
    "from torch.optim import AdamW\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "train_loader = train_data_loader\n",
    "val_loader   = val_data_loader\n",
    "test_loader  = test_data_loader\n",
    "\n",
    "# Optimizer: AdamW is the recommended optimizer for BERT\n",
    "optimizer = AdamW(model.parameters(), lr=PARAMS[\"learning_rate\"])\n",
    "\n",
    "# Scheduler: adjusts the learning rate during training\n",
    "total_steps = len(train_loader) * PARAMS[\"epochs\"]\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,          # warmup can be tuned\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# Loss function: CrossEntropy for multi-class classification (2 classes: FAKE/TRUE)\n",
    "criterion = CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c194ba6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_loader)\n",
    "print(optimizer)\n",
    "print(scheduler)\n",
    "print(criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357fba60",
   "metadata": {},
   "source": [
    "#### **Step 6. Training Loop**\n",
    "\n",
    "This step is where the model actually **learns** from data. During training:  \n",
    "1. Batches of text + labels are fed into the model.  \n",
    "2. The model makes predictions.  \n",
    "3. A **loss** is calculated to measure errors.  \n",
    "4. Backpropagation updates the model weights to improve performance.  \n",
    "\n",
    "We repeat this process over several **epochs** (full passes through the training set).  \n",
    "At the end of each epoch, we track:  \n",
    " **Average loss** → how well the model is fitting.  \n",
    " **Accuracy** → how well it’s predicting.  \n",
    "\n",
    "This step is the core of fine-tuning BERT for our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c734c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def train_epoch(model, data_loader, optimizer, criterion, scheduler, device, epoch=None, log_interval=50):\n",
    "    model = model.train()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    total = 0\n",
    "\n",
    "    progress = tqdm(enumerate(data_loader), total=len(data_loader), desc=f\"Epoch {epoch} [Train]\", leave=False)\n",
    "\n",
    "    for step, batch in progress:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)  \n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs\n",
    "        _, preds = torch.max(logits, dim=1)\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        correct_predictions += torch.sum(preds == labels).item()\n",
    "        total += labels.size(0)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Update tqdm bar with running loss and accuracy\n",
    "        if (step + 1) % log_interval == 0 or (step + 1) == len(data_loader):\n",
    "            avg_loss = np.mean(losses)\n",
    "            acc = correct_predictions / total\n",
    "            progress.set_postfix({\"loss\": f\"{avg_loss:.4f}\", \"acc\": f\"{acc:.4f}\"})\n",
    "\n",
    "    return correct_predictions / total, np.mean(losses)\n",
    "\n",
    "\n",
    "def eval_model(model, data_loader, criterion, device, epoch=None):\n",
    "    model = model.eval()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    total = 0\n",
    "\n",
    "    progress = tqdm(enumerate(data_loader), total=len(data_loader), desc=f\"Epoch {epoch} [Eval]\", leave=False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for step, batch in progress:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)   \n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs\n",
    "            _, preds = torch.max(logits, dim=1)\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            correct_predictions += torch.sum(preds == labels).item()\n",
    "            total += labels.size(0)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            if (step + 1) % 50 == 0 or (step + 1) == len(data_loader):\n",
    "                avg_loss = np.mean(losses)\n",
    "                acc = correct_predictions / total\n",
    "                progress.set_postfix({\"loss\": f\"{avg_loss:.4f}\", \"acc\": f\"{acc:.4f}\"})\n",
    "\n",
    "    return correct_predictions / total, np.mean(losses)\n",
    "\n",
    "# Main Training Loop\n",
    "history = {\"train_acc\": [], \"train_loss\": [], \"val_acc\": [], \"val_loss\": []}\n",
    "\n",
    "for epoch in range(PARAMS[\"epochs\"]):\n",
    "    print(f\"Epoch {epoch + 1}/{PARAMS['epochs']}\")\n",
    "\n",
    "    train_acc, train_loss = train_epoch(\n",
    "        model, train_data_loader, optimizer, criterion, scheduler, device, epoch=epoch+1\n",
    "    )\n",
    "    val_acc, val_loss = eval_model(\n",
    "        model, val_data_loader, criterion, device, epoch=epoch+1\n",
    "    )\n",
    "\n",
    "    history[\"train_acc\"].append(train_acc)\n",
    "    history[\"train_loss\"].append(train_loss)\n",
    "    history[\"val_acc\"].append(val_acc)\n",
    "    history[\"val_loss\"].append(val_loss)\n",
    "\n",
    "    print(f\"Train loss {train_loss:.4f}, accuracy {train_acc:.4f}\")\n",
    "    print(f\"Val   loss {val_loss:.4f}, accuracy {val_acc:.4f}\")\n",
    "    print(\"-\" * 50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9755c057",
   "metadata": {},
   "source": [
    "**Training Epoch Outputs**\n",
    "\n",
    "Each epoch output shows the model's performance on both the training and validation sets:  \n",
    "\n",
    " **Train loss / accuracy**: How well the model is fitting the training data. A decreasing loss and increasing accuracy indicate learning.  \n",
    " **Validation loss / accuracy**: How well the model generalizes to unseen data. Stable or slightly higher validation loss compared to training is normal.  \n",
    " **Epoch progression**: Each epoch represents a full pass through the training dataset.  \n",
    "\n",
    "From the outputs, the model quickly learned to distinguish FAKE and TRUE news, achieving very high accuracy and low loss by the final epoch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d716be4f",
   "metadata": {},
   "source": [
    "#### **Step 7. Evaluation**\n",
    "\n",
    "Now that our BERT model has been trained for 3 epochs, we need to evaluate its performance on the test set.  \n",
    "This involves:\n",
    " Switching the model to **evaluation mode** (disables dropout, gradient updates).  \n",
    " Running the model on the test dataloader.  \n",
    " Collecting predictions and true labels.  \n",
    " Computing evaluation metrics:\n",
    "   **Accuracy** (overall correctness).  \n",
    "   **Precision, Recall, F1-score** (per-class and averaged).  \n",
    "   **Confusion Matrix** (to see where the model makes mistakes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a756c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_data_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        # HuggingFace model returns logits directly\n",
    "        logits = outputs.logits if hasattr(outputs, \"logits\") else outputs\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Accuracy\n",
    "acc = accuracy_score(all_labels, all_preds)\n",
    "print(f\"Test Accuracy: {acc:.4f}\")\n",
    "\n",
    "# Detailed metrics\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=[\"FAKE\", \"TRUE\"]))\n",
    "\n",
    "y_pred_bert_combined = np.array(all_preds)\n",
    "y_true_bert_combined = np.array(all_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2acb5bd",
   "metadata": {},
   "source": [
    "The model achieved outstanding performance on the test set:\n",
    "\n",
    " **Accuracy:** 99.96%  \n",
    " **Precision, Recall, and F1-score:** All values are essentially 1.00 (100%) for both classes (FAKE and TRUE).  \n",
    "The test set contained 2,348 FAKE and 2,142 TRUE examples, meaning the dataset is relatively balanced.  \n",
    "\n",
    "####  Interpretation\n",
    "The model is almost perfectly distinguishing between FAKE and TRUE news articles.  \n",
    " An accuracy of 99.96% suggests that only about **2 samples out of 4,490** were misclassified.  \n",
    " High precision and recall indicate that the model is not only making correct predictions but also covering nearly all true cases.  \n",
    "\n",
    "####  Conclusion\n",
    "These results show that the fine-tuned BERT model generalizes very well on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a16cb85",
   "metadata": {},
   "source": [
    "### **4.8 Model and Evaluation Metrics Comparison**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c07038",
   "metadata": {},
   "source": [
    "We define a function that creates a table that compares all the different model metrics and export the table to ensure we don't have to rerun on the GPU again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00316edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Unified artifact saver: writes to Kaggle (if present) and to local ../data when available ---\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def save_artifact(df, filename: str):\n",
    "    \"\"\"\n",
    "    Save a pandas DataFrame to:\n",
    "      - /kaggle/working/<filename> when running in Kaggle\n",
    "      - ../data/<filename> if ../data exists (local runs)\n",
    "    Returns the paths that were actually written.\n",
    "    \"\"\"\n",
    "    written = []\n",
    "    # 1) Kaggle working dir\n",
    "    kaggle_dir = Path(\"/kaggle/working\")\n",
    "    if kaggle_dir.exists():\n",
    "        out_k = kaggle_dir / filename\n",
    "        df.to_csv(out_k, index=False)\n",
    "        print(f\"[Kaggle] Saved: {out_k}\")\n",
    "        written.append(str(out_k))\n",
    "\n",
    "    # 2) Local ../data (if present)\n",
    "    local_dir = Path(\"../data\")\n",
    "    if local_dir.exists():\n",
    "        out_l = local_dir / filename\n",
    "        df.to_csv(out_l, index=False)\n",
    "        print(f\"[Local]  Saved: {out_l}\")\n",
    "        written.append(str(out_l))\n",
    "\n",
    "    # 3) Fallback: current directory\n",
    "    if not written:\n",
    "        out_f = Path(\"./\") / filename\n",
    "        df.to_csv(out_f, index=False)\n",
    "        print(f\"[Fallback] Saved: {out_f}\")\n",
    "        written.append(str(out_f))\n",
    "\n",
    "    return written"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941dddbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import pandas as pd\n",
    "\n",
    "# Helper: coerce any y (strings, bools, logits, probs, tensors) -> int IDs using your existing lbl_enc/id2label\n",
    "def to_label_ids(y, lbl_enc, id2label=None):\n",
    "    # Convert tensors to numpy\n",
    "    if hasattr(y, \"detach\"):  # torch tensor\n",
    "        y = y.detach().cpu().numpy()\n",
    "    y = np.asarray(y)\n",
    "\n",
    "    # If y are logits/probabilities shape [N, C], argmax to class ids\n",
    "    if y.ndim == 2:\n",
    "        y = y.argmax(axis=1)\n",
    "\n",
    "    # If booleans, cast to int\n",
    "    if y.dtype == np.bool_:\n",
    "        return y.astype(np.int64)\n",
    "\n",
    "    # If already ints, just cast\n",
    "    if np.issubdtype(y.dtype, np.integer):\n",
    "        return y.astype(np.int64)\n",
    "\n",
    "    # Handle strings/object labels -> use lbl_enc to map to ids\n",
    "    y_str = y.astype(str)\n",
    "    try:\n",
    "        return lbl_enc.transform(y_str).astype(np.int64)\n",
    "    except Exception:\n",
    "        # Fallback via id2label mapping if provided\n",
    "        if id2label is None:\n",
    "            raise\n",
    "        label2id_fallback = {v: k for k, v in id2label.items()}\n",
    "        return np.array([label2id_fallback[s] for s in y_str], dtype=np.int64)\n",
    "\n",
    "def row(model, variant, y_true, y_pred):\n",
    "    acc  = accuracy_score(y_true, y_pred)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average=\"weighted\", zero_division=0\n",
    "    )\n",
    "    return {\"model\": model, \"variant\": variant,\n",
    "            \"accuracy\": acc, \"precision\": prec, \"recall\": rec, \"f1\": f1}\n",
    "\n",
    "rows = []\n",
    "\n",
    "# --- Normalize all y_* to integer IDs consistently ---\n",
    "# Ground truths\n",
    "y_true_lr          = to_label_ids(y_test,                 lbl_enc, id2label)\n",
    "y_true_lstm_clean  = to_label_ids(y_te,                   lbl_enc, id2label)       # if your LSTM-clean uses y_te\n",
    "y_true_lstm_comb   = to_label_ids(y_test,                 lbl_enc, id2label)\n",
    "y_true_bert_c      = to_label_ids(y_true_bert_clean,      lbl_enc, id2label)\n",
    "y_true_bert_comb   = to_label_ids(y_true_bert_combined,   lbl_enc, id2label)\n",
    "\n",
    "# Predictions (handle strings, tensors, or logits)\n",
    "y_pred_lr_clean_id   = to_label_ids(y_pred_lr_clean,        lbl_enc, id2label)\n",
    "y_pred_lr_comb_id    = to_label_ids(y_pred_lr_combined,     lbl_enc, id2label)\n",
    "\n",
    "y_pred_lstm_clean_id = to_label_ids(y_pred_lstm_clean,      lbl_enc, id2label)\n",
    "y_pred_lstm_comb_id  = to_label_ids(y_pred_lstm_combined,   lbl_enc, id2label)\n",
    "\n",
    "y_pred_bert_c_id     = to_label_ids(y_pred_bert_clean,      lbl_enc, id2label)\n",
    "y_pred_bert_comb_id  = to_label_ids(y_pred_bert_combined,   lbl_enc, id2label)\n",
    "\n",
    "# (Optional) sanity: the label sets should match\n",
    "assert set(np.unique(y_true_lr)) <= set(lbl_enc.transform(lbl_enc.classes_)), \"Unexpected y_true labels\"\n",
    "\n",
    "# --- Build the table using ONLY normalized arrays ---\n",
    "rows.append(row(\"LogReg\", \"clean\",    y_true_lr,         y_pred_lr_clean_id))\n",
    "rows.append(row(\"LogReg\", \"combined\", y_true_lr,         y_pred_lr_comb_id))\n",
    "\n",
    "rows.append(row(\"LSTM\",   \"clean\",    y_true_lstm_clean, y_pred_lstm_clean_id))\n",
    "rows.append(row(\"LSTM\",   \"combined\", y_true_lstm_comb,  y_pred_lstm_comb_id))\n",
    "\n",
    "rows.append(row(\"BERT\",   \"clean\",    y_true_bert_c,     y_pred_bert_c_id))\n",
    "rows.append(row(\"BERT\",   \"combined\", y_true_bert_comb,  y_pred_bert_comb_id))\n",
    "\n",
    "metrics_df = pd.DataFrame(rows).sort_values([\"model\",\"variant\"])\n",
    "display(metrics_df.sort_values(\"f1\", ascending=False))\n",
    "\n",
    "out_path = RESULTS_OUT / \"model_comparison_clean_vs_combined.csv\"\n",
    "save_artifact(metrics_df, \"model_comparison_clean_vs_combined.csv\")\n",
    "print(f\"Saved metrics table to: {out_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
