{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "with zipfile.ZipFile(\"../data/archive.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"unzipped_data\")\n",
    "    \n",
    "print(\"Files extracted successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "fake_df = pd.read_csv(\"unzipped_data/Fake.csv\")\n",
    "true_df = pd.read_csv(\"unzipped_data/True.csv\")\n",
    "\n",
    "print(\"Fake News Dataset:\", fake_df.shape)\n",
    "print(\"True News Dataset:\", true_df.shape)\n",
    "\n",
    "fake_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge and label\n",
    "\n",
    "#Add a label column\n",
    "fake_df[\"label\"] = \"FAKE\"\n",
    "true_df[\"label\"] = \"TRUE\"\n",
    "\n",
    "#Merge into one dataset\n",
    "data = pd.concat([fake_df, true_df], ignore_index = True)\n",
    "\n",
    "#Shuffle the rows so FAKE and TRUE are mixed\n",
    "data = data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "#Check the structure\n",
    "print(data.shape)\n",
    "print(data[\"label\"].value_counts())\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling\n",
    "## Logistic Regression - Baseline Model\n",
    "\n",
    "Logistic Regression (TF-IDF) on merged dataset\n",
    "\n",
    "We train a baseline Logistic Regression model using TF-IDF features. Input: data with columns like title, text, label where label (\"FAKE\",\"TRUE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, accuracy_score,\n",
    "    roc_auc_score, roc_curve, ConfusionMatrixDisplay\n",
    ")\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 1:\n",
    "We'll Encode labels and pick features\n",
    "* Encode FAKE- 1, TRUE- 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels: FAKE = 1, TRUE = 0\n",
    "data[\"label\"] = data[\"label\"].map({\"FAKE\": 1, \"TRUE\": 0})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2:  Feature/Target Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[\"text\"]      # text\n",
    "y = data[\"label\"]     # numeric target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Train/Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: TF-IDF Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features=5000, stop_words=\"english\")\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Train Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg = LogisticRegression(max_iter=1000, solver=\"liblinear\")\n",
    "log_reg.fit(X_train_tfidf, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Evaluate\n",
    "* accuracy, report, confusion matrix, ROC-AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted labels and probabilities\n",
    "y_pred = log_reg.predict(X_test_tfidf)\n",
    "y_proba = log_reg.predict_proba(X_test_tfidf)[:, 1]   # probability of FAKE = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy:\", round(accuracy_score(y_test, y_pred), 4))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred, target_names=[\"TRUE\",\"FAKE\"]))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "ConfusionMatrixDisplay(cm, display_labels=[\"TRUE\",\"FAKE\"]).plot(values_format=\"d\")\n",
    "plt.title(\"Logistic Regression — Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "auc = roc_auc_score(y_test, y_proba)\n",
    "fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "plt.plot(fpr, tpr, label=f\"ROC-AUC = {auc:.3f}\")\n",
    "plt.plot([0,1],[0,1],\"--\")\n",
    "plt.xlabel(\"False Positive Rate\"); plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Logistic Regression — ROC Curve\"); plt.legend(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Performance Summary\n",
    "\n",
    "Accuracy: 0.987 (~99%)\n",
    "\n",
    "Precision: TRUE = 0.98, FAKE = 0.99\n",
    "\n",
    "Recall: TRUE = 0.99, FAKE = 0.98\n",
    "\n",
    "F1-score: Both classes ~0.99\n",
    "\n",
    "ROC-AUC: 0.999 (excellent separation between classes)\n",
    "\n",
    "\n",
    "### Confusion Matrix Insights\n",
    "\n",
    "TRUE articles: 4241 correctly predicted, 43 misclassified as FAKE.\n",
    "\n",
    "FAKE articles: 4619 correctly predicted, 77 misclassified as TRUE.\n",
    "\n",
    "The model makes very few mistakes compared to the large sample size.\n",
    "\n",
    "* Interpretation:*\n",
    "\n",
    "Errors are balanced between both classes - the model is not biased towards TRUE or FAKE.\n",
    "\n",
    "Misclassifications (only ~120 out of ~9000) are acceptable in text classification at this stage.\n",
    "\n",
    "### ROC Curve\n",
    "\n",
    "ROC-AUC = 0.999 - that means the model can almost perfectly distinguish between fake and true news.\n",
    "\n",
    "The curve hugs the top-left corner which is an indication of very high sensitivity and specificity.\n",
    "\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "Strong baseline: Even without tuning, TF-IDF + Logistic Regression performs extremely well.\n",
    "\n",
    "Low error rate: Only ~1% of articles are misclassified.\n",
    "\n",
    "Next steps: Hyperparameter tuning (GridSearchCV) to confirm robustness and maybe squeeze out minor improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save artifacts (model + vectorizer) for reuse/deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, joblib\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "joblib.dump(log_reg, \"models/logreg_model.joblib\")\n",
    "joblib.dump(tfidf,   \"models/tfidf_vectorizer.joblib\")\n",
    "print(\"Saved: models/logreg_model.joblib, models/tfidf_vectorizer.joblib\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps - (Hyperparameter Tuning with GridSearchCV)\n",
    "\n",
    "Now that we have a strong baseline, we perform hyperparameter tuning to confirm robustness and test whether performance can be further optimized.\n",
    "\n",
    "We tune Logistic Regression parameters using GridSearchCV:\n",
    "\n",
    "- `tfidf__ngram_range`: unigrams vs bigrams  \n",
    "- `tfidf__max_df` and `tfidf__min_df`: filter overly common/rare words  \n",
    "- `clf__C`: regularization strength  \n",
    "- `clf__solver` and `clf__penalty`: logistic regression optimization  \n",
    "\n",
    "Evaluation metric: **F1-macro** (balances FAKE and TRUE equally).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "\n",
    "# Pipeline: TF-IDF + Logistic Regression\n",
    "pipe = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(stop_words=\"english\")),\n",
    "    (\"clf\", LogisticRegression(max_iter=2000, random_state=42))\n",
    "])\n",
    "\n",
    "# Parameter grid\n",
    "param_grid = {\n",
    "    \"tfidf__max_df\": [0.5, 0.7, 0.9],\n",
    "    \"tfidf__min_df\": [2, 5],\n",
    "    \"tfidf__ngram_range\": [(1,1), (1,2)],\n",
    "    \"clf__C\": [0.1, 1, 3, 10],\n",
    "    \"clf__solver\": [\"liblinear\"],\n",
    "    \"clf__penalty\": [\"l2\"]\n",
    "}\n",
    "\n",
    "# Cross-validation strategy\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Run GridSearch\n",
    "gs = GridSearchCV(pipe, param_grid, scoring=\"f1_macro\", cv=cv, n_jobs=-1, verbose=1)\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best CV f1_macro:\", round(gs.best_score_, 4))\n",
    "print(\"Best params:\", gs.best_params_)\n",
    "\n",
    "# Evaluate best model on test set\n",
    "best = gs.best_estimator_\n",
    "pred = best.predict(X_test)\n",
    "proba = best.predict_proba(X_test)[:,1]\n",
    "\n",
    "print(\"\\nTest report (best):\\n\", classification_report(y_test, pred, target_names=[\"TRUE\",\"FAKE\"]))\n",
    "print(\"Test ROC-AUC:\", round(roc_auc_score(y_test, proba), 4))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning Results\n",
    "\n",
    "GridSearchCV tested **48 candidate parameter combinations**, each evaluated with **5-fold cross-validation**, totalling **240 fits**.  \n",
    "This process helps ensure the best model is not chosen by chance and generalizes well.\n",
    "\n",
    "#### Key Outputs:\n",
    "- **Best Parameters (`gs.best_params_`)**  \n",
    "  Shows the optimal settings for TF-IDF and Logistic Regression (e.g., n-grams, min/max document frequency, and regularization strength).  \n",
    "  We found:\n",
    "  * clf_ _C = 10\n",
    "  * clf_ _penalty =12\n",
    "  * clf_ _solver = lilinear\n",
    "  * tfidf_ _max_df = 0.5\n",
    "  * tfidf_ _min_df =5\n",
    "  * tfidf_ _ngram_range = (1, 2)\n",
    "\n",
    "- **Best Cross-Validated Score (CV F1-macro):** 0.99\n",
    "- confirms chosen parameters perform consistently across folds.\n",
    "\n",
    "- **Test Set Evaluation (Best Model):**    \n",
    "  - **Classification Report** (precision, recall, F1 for bothTRUE/FAKE)  \n",
    "  - **ROC-AUC Score** 0.99 (excellent probability-based performance)\n",
    "\n",
    "#### Interpretation:\n",
    "- The tuned model gave almost the same results as the baseline (~0.99).\n",
    "This shows that the baseline Logistic Regression was already close to optimal.\n",
    "- Running GridSearchCV was still valuable because it confirmed the model's stability and robustness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Tuned Model Artifacts - For Deployment \n",
    "\n",
    "After hyperparameter tuning, we save the best model and vectorizer.  \n",
    "These artifacts will be reused in deployment (Streamlit app) and reporting (Tableau dashboard).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, joblib\n",
    "\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "# Save tuned model and vectorizer\n",
    "joblib.dump(best, \"models/logreg_tuned_model.joblib\")\n",
    "joblib.dump(best.named_steps[\"tfidf\"], \"models/tfidf_tuned_vectorizer.joblib\")\n",
    "\n",
    "print(\"Saved: models/logreg_tuned_model.joblib, models/tfidf_tuned_vectorizer.joblib\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline vs Tuned Model Comparison\n",
    "\n",
    "We compare baseline Logistic Regression with the tuned GridSearchCV model to highlight improvements.\n",
    "\n",
    "| Model                      | Accuracy | F1 (Macro) | ROC-AUC |\n",
    "|-----------------------------|----------|------------|---------|\n",
    "| Logistic Regression (Base)  | 0.9866   | ~0.9866    | ~0.999  |\n",
    "| Logistic Regression (Tuned) | 0.9899   | ~0.9899   | ~0.9997 |\n",
    "\n",
    "**Interpretation:**\n",
    "- The baseline model already performs very well.\n",
    "- Hyperparameter tuning slightly improves F1/ROC-AUC, showing robustness.\n",
    "- Either model could be deployed, but the tuned model is safer for production since parameters were validated across multiple folds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ds38)",
   "language": "python",
   "name": "ds38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
