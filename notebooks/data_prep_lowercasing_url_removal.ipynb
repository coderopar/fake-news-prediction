{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62506671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fake_news_text_preprocessing.py\n",
    "# Text Preprocessing Module - Lowercasing & URL Removal\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Set display options for better readability\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 300)\n",
    "\n",
    "# Set matplotlib style safely\n",
    "try:\n",
    "    plt.style.use('seaborn-v0_8')\n",
    "except:\n",
    "    plt.style.use('seaborn')  # Fallback for older versions\n",
    "    print(\"Using seaborn style (seaborn-v0_8 not available)\")\n",
    "\n",
    "print(\"=== FAKE NEWS DETECTION - TEXT PREPROCESSING MODULE ===\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754cf451",
   "metadata": {},
   "source": [
    "## 1. DATA LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd361094",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"1. LOADING AND PREPARING DATASET...\")\n",
    "\n",
    "try:\n",
    "    # Extract dataset from zip file\n",
    "    print(\"Extracting dataset from archive...\")\n",
    "    with zipfile.ZipFile(\"../data/archive.zip\", 'r') as zip_ref:\n",
    "        zip_ref.extractall(\"unzipped_data\")\n",
    "    print(\"‚úì Dataset extracted successfully!\")\n",
    "    \n",
    "    # Load the datasets\n",
    "    df_fake = pd.read_csv(\"unzipped_data/Fake.csv\")\n",
    "    df_true = pd.read_csv(\"unzipped_data/True.csv\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ö† Zip file not found. Trying to load CSV files directly...\")\n",
    "    try:\n",
    "        df_fake = pd.read_csv(\"Fake.csv\")\n",
    "        df_true = pd.read_csv(\"True.csv\")\n",
    "        print(\"‚úì CSV files loaded directly!\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"‚ùå Error: Could not find dataset files.\")\n",
    "        print(\"Please ensure the dataset files are in the correct location.\")\n",
    "        exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d076d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add labels to distinguish between fake and real news\n",
    "df_fake[\"label\"] = \"FAKE\"\n",
    "df_true[\"label\"] = \"TRUE\"\n",
    "\n",
    "# Combine into one dataset for processing\n",
    "df = pd.concat([df_fake, df_true], axis=0).reset_index(drop=True)\n",
    "\n",
    "print(f\"‚úì Dataset loaded successfully!\")\n",
    "print(f\"   - Total articles: {len(df)}\")\n",
    "print(f\"   - Fake articles: {len(df_fake)}\")\n",
    "print(f\"   - Real articles: {len(df_true)}\")\n",
    "print(f\"   - Columns: {list(df.columns)}\\n\")\n",
    "\n",
    "# Display sample of the data\n",
    "print(\"Sample of original data:\")\n",
    "print(df[['title', 'text', 'label']].head(2))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9625832f",
   "metadata": {},
   "source": [
    "## 2. TEXT PREPROCESSING FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b95022a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text_lowercase_url(text):\n",
    "    \"\"\"\n",
    "    MAIN PREPROCESSING FUNCTION:\n",
    "    - Converts text to lowercase\n",
    "    - Removes URLs, hyperlinks, and website addresses\n",
    "    - Handles missing values safely\n",
    "    - Cleans extra whitespace\n",
    "    \"\"\"\n",
    "    # Handle missing values\n",
    "    if pd.isna(text) or text is None:\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to string to ensure consistent processing\n",
    "    text = str(text)\n",
    "    \n",
    "    # COMPREHENSIVE URL REMOVAL PATTERN:\n",
    "    url_pattern = r'https?://\\S+|www\\.\\S+|\\S+\\.(com|org|net|edu|gov|io|co|uk)\\S*|bit\\.ly/\\S+|t\\.co/\\S+'\n",
    "    \n",
    "    # Remove all URLs from text\n",
    "    text = re.sub(url_pattern, '', text)\n",
    "    \n",
    "    # Convert entire text to lowercase for consistency\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Clean up extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "print(\"‚úì Preprocessing functions defined!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9018341",
   "metadata": {},
   "source": [
    "## 3. APPLY PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f89b040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create copies of original text for comparison\n",
    "df['title_original'] = df['title'].copy()\n",
    "df['text_original'] = df['text'].copy()\n",
    "\n",
    "# Apply preprocessing to title and text columns\n",
    "print(\"Processing titles...\")\n",
    "df['title_clean'] = df['title'].apply(preprocess_text_lowercase_url)\n",
    "\n",
    "print(\"Processing article texts...\")\n",
    "df['text_clean'] = df['text'].apply(preprocess_text_lowercase_url)\n",
    "\n",
    "print(\"‚úì Text preprocessing completed!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27a9472",
   "metadata": {},
   "source": [
    "## 4. QUALITY CONTROL AND VERIFICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84e6b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_url(text):\n",
    "    \"\"\"Check if text contains any URLs\"\"\"\n",
    "    url_pattern = r'https?://|www\\.|\\.[a-z]{2,}'\n",
    "    return bool(re.search(url_pattern, str(text).lower()))\n",
    "\n",
    "def count_uppercase(text):\n",
    "    \"\"\"Count uppercase characters in text\"\"\"\n",
    "    return sum(1 for char in str(text) if char.isupper())\n",
    "\n",
    "# Count URLs in original vs cleaned text\n",
    "urls_original_title = df['title_original'].apply(contains_url).sum()\n",
    "urls_clean_title = df['title_clean'].apply(contains_url).sum()\n",
    "urls_original_text = df['text_original'].apply(contains_url).sum()\n",
    "urls_clean_text = df['text_clean'].apply(contains_url).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7992d5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check case conversion\n",
    "uppercase_original = df['text_original'].apply(count_uppercase).sum()\n",
    "uppercase_clean = df['text_clean'].apply(count_uppercase).sum()\n",
    "\n",
    "# Verify no data loss occurred\n",
    "original_non_empty = df['text_original'].apply(lambda x: len(str(x)) > 0).sum()\n",
    "clean_non_empty = df['text_clean'].apply(lambda x: len(str(x)) > 0).sum()\n",
    "\n",
    "print(\"URL Removal Results:\")\n",
    "print(f\"  Titles: {urls_original_title} URLs ‚Üí {urls_clean_title} URLs remaining\")\n",
    "print(f\"  Texts:  {urls_original_text} URLs ‚Üí {urls_clean_text} URLs remaining\")\n",
    "print(f\"  Total URLs removed: {urls_original_title + urls_original_text - urls_clean_title - urls_clean_text}\")\n",
    "\n",
    "print(\"\\nCase Conversion Results:\")\n",
    "print(f\"  Uppercase characters: {uppercase_original} ‚Üí {uppercase_clean}\")\n",
    "print(f\"  Reduction: {uppercase_original - uppercase_clean} characters\")\n",
    "\n",
    "print(\"\\nData Integrity Check:\")\n",
    "print(f\"  Non-empty original texts: {original_non_empty}\")\n",
    "print(f\"  Non-empty cleaned texts: {clean_non_empty}\")\n",
    "\n",
    "if original_non_empty == clean_non_empty:\n",
    "    print(\"‚úì No data loss detected!\\n\")\n",
    "else:\n",
    "    print(\"‚ö† Warning: Possible data loss detected!\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b03b73",
   "metadata": {},
   "source": [
    "## 5. SHOW SAMPLE RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87d549f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"BEFORE PREPROCESSING (First article):\")\n",
    "print(\"-\" * 50)\n",
    "print(\"Title:\", df['title_original'].iloc[0][:100] + \"...\" if len(str(df['title_original'].iloc[0])) > 100 else df['title_original'].iloc[0])\n",
    "print(\"Text:\", df['text_original'].iloc[0][:200] + \"...\" if len(str(df['text_original'].iloc[0])) > 200 else df['text_original'].iloc[0])\n",
    "\n",
    "print(\"\\nAFTER PREPROCESSING (First article):\")\n",
    "print(\"-\" * 50)\n",
    "print(\"Title:\", df['title_clean'].iloc[0][:100] + \"...\" if len(str(df['title_clean'].iloc[0])) > 100 else df['title_clean'].iloc[0])\n",
    "print(\"Text:\", df['text_clean'].iloc[0][:200] + \"...\" if len(str(df['text_clean'].iloc[0])) > 200 else df['text_clean'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe052812",
   "metadata": {},
   "source": [
    "## 6. SIMPLE VISUALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c814c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Create a simple visualization\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # URL removal comparison\n",
    "    categories = ['Titles', 'Texts']\n",
    "    original_urls = [urls_original_title, urls_original_text]\n",
    "    clean_urls = [urls_clean_title, urls_clean_text]\n",
    "    \n",
    "    x = np.arange(len(categories))\n",
    "    width = 0.35\n",
    "    \n",
    "    ax1.bar(x - width/2, original_urls, width, label='Original', color='red', alpha=0.7)\n",
    "    ax1.bar(x + width/2, clean_urls, width, label='Cleaned', color='green', alpha=0.7)\n",
    "    ax1.set_title('URL Removal Effectiveness')\n",
    "    ax1.set_ylabel('Number of URLs')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(categories)\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Uppercase reduction\n",
    "    case_categories = ['Original', 'Cleaned']\n",
    "    case_counts = [uppercase_original, uppercase_clean]\n",
    "    \n",
    "    ax2.bar(case_categories, case_counts, color=['blue', 'orange'], alpha=0.7)\n",
    "    ax2.set_title('Uppercase Character Reduction')\n",
    "    ax2.set_ylabel('Total Uppercase Characters')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('preprocessing_results.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"‚úì Visualization saved as 'preprocessing_results.png'\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö† Visualization failed: {e}\")\n",
    "    print(\"Continuing without visualization...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1da9a97",
   "metadata": {},
   "source": [
    "## 7. SAVE PROCESSED DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10f50a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n7. SAVING PROCESSED DATA...\")\n",
    "\n",
    "# Save the processed dataset\n",
    "output_filename = 'fake_news_preprocessed.csv'\n",
    "df.to_csv(output_filename, index=False)\n",
    "\n",
    "print(f\"‚úì Processed data saved as '{output_filename}'\")\n",
    "print(f\"‚úì File contains {len(df)} articles\")\n",
    "\n",
    "# Save only the essential columns for the next pipeline stage\n",
    "essential_columns = ['title_clean', 'text_clean', 'label']\n",
    "df[essential_columns].to_csv('fake_news_clean.csv', index=False)\n",
    "print(f\"‚úì Cleaned data saved as 'fake_news_clean.csv' (essential columns only)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948edba1",
   "metadata": {},
   "source": [
    "## 8. FINAL SUMMARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0071f440",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PREPROCESSING COMPLETE - SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"‚úÖ TASKS COMPLETED:\")\n",
    "print(\"   - URLs and hyperlinks successfully removed\")\n",
    "print(\"   - All text converted to lowercase\")\n",
    "print(\"   - Data integrity maintained\")\n",
    "print(\"   - Processed data exported\")\n",
    "\n",
    "print(\"\\nüìä KEY RESULTS:\")\n",
    "print(f\"   - URLs removed: {urls_original_title + urls_original_text - urls_clean_title - urls_clean_text}\")\n",
    "print(f\"   - Uppercase characters reduced: {uppercase_original - uppercase_clean}\")\n",
    "print(f\"   - Articles processed: {len(df)}\")\n",
    "\n",
    "print(\"\\nüìÅ OUTPUT FILES:\")\n",
    "print(\"   - fake_news_preprocessed.csv (full dataset with original + cleaned)\")\n",
    "print(\"   - fake_news_clean.csv (cleaned data only, for next stage)\")\n",
    "print(\"   - preprocessing_results.png (results visualization)\")\n",
    "\n",
    "print(\"\\nüéØ NEXT STEPS:\")\n",
    "print(\"   - Data is ready for tokenization and further NLP processing\")\n",
    "print(\"   - Pass 'fake_news_clean.csv' to the next team member\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
